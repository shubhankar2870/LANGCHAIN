{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhankarkeshri/Developer/LangChain/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 0}, page_content='1 \\n Generative chemistry: drug discovery with deep learning generative models  \\n \\nYuemin Bian1,2 and Xiang -Qun Xie1,2,3,4* \\n \\n1Department of Pharmaceutical Sciences and Computational Chemical Genomics Screening Center, School of \\nPharmacy; 2NIH National Center of Exc ellence for Computational Drug Abuse Research; 3Drug Discovery \\nInstitute; 4Departments of Computational Biology and Structural Biology, School of Medicine, University of \\nPittsburgh, Pittsburgh, Pennsylvania 15261, United States.  \\n \\n \\n*Corresponding Author: Xi ang-Qun Xie, MBA, Ph.D.  \\nProfessor of Pharmaceutical Sciences/Drug Discovery Institute  \\nDirector of CCGS and NIDA CDAR Centers  \\n335 Sutherland Drive, 206 Salk Pavilion  \\nUniversity of Pittsburgh  \\nPittsburgh, PA15261, USA  \\n412-383-5276 (Phone)  \\n412-383-7436 (Fax)  \\nEmail: xix15@pitt.edu  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 1}, page_content='2 \\n Abstract :  \\nThe de novo  design of molecular structures using deep learning generative models introduces an encouraging \\nsolution to drug discovery in the face of the continuously increased cost of new drug development. From the \\ngenera tion of original texts, images, and videos, to the scratching of novel molecular structures, the incredible  \\ncreativity of deep learning generative models surprise d us about the height machine intelligence can achieve. \\nThe purpose of this paper is to review  the latest advances in generative chemistry which relies on generative \\nmodeling to expedite the drug discovery process.  This review starts with a brief history of artificial intelligence  \\nin drug discovery to outline this emerging paradigm. Commonly used c hemical databases, molecular \\nrepresentations, and tools in  cheminformatics and machine learning are covered as the infrastructure  for the \\ngenerative chemistry . The detailed discussion s on utilizing  cutting -edge generative architectures,  including  \\nrecurrent  neural network , variational autoencoder , adversarial autoencoder , and generative adversarial network  \\nfor compound generation are focused . Challenges and f uture perspectives  follow .  \\n \\nKeywords : Drug discovery, Deep learning, Generative model, Recurrent neu ral network, Variation al \\nautoencoder, Adversarial autoencoder, Generative adversarial network  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='3 \\n 1. INTRODUCTION  \\nDrug discovery is expensive. The average cost for the development of a new drug now hits 2.6 billion USD and \\nthe overall discovery process takes over 12 years to finish1, 2. Moreover, these numbers keep  increasing. It is \\ncritical to think and explore efficient and effective strategies t o confront the growing cost and to accelerate the \\ndiscovery process. The progression in the high -throughput screening (HTS) dramatically speeded up the task of \\nlead identification by screening candidate compounds in large volume3, 4. When it comes to the lead \\nidentification, the concept can be further classified into two divisions, the structure -based approach5, 6 and the \\nligand -based approach7. Combining  with the significant progress  in computation, the development of thes e two \\napproaches has resulted in constructive virtual screening (VS) methodologies. Traditionally, with the structure \\nof the target protein available, structure -based approaches including molecular docking studies8-10, molecular \\ndynamic simulations11-13, fragment -based approach10, 14, etc. can be applied to explore the potential receptor -\\nligand interactions and to virtually screen a large compound set for finding the plausible lead. Then with the \\nidentified active molecules for the given target, ligand -based approaches such as pharmac ophore modeling15, 16, \\nscaffolding hopping17, 18, and molecular fingerprint similarity search19 can be conducted for modifying known \\nleads and f or finding future compounds. The rapid advancement in computational power and the blossom of \\nmachine learning (ML) algorithms brought the ML -based decision -making model20, 21 as an alternative path to \\nthe VS campaigns in the past decades. There is increased availability of data in cheminformatics and drug \\ndiscovery. The capability of dealing with large data  to dete ct hidden patterns and to facilitate the future data \\nprediction in a time -efficient manner  favored ML in building VS pipelines.  \\n It is encouraging to note t he successful applications of the above -mentioned computational chemistry \\napproaches and ML -based V S pipelines on drug discovery these days. The conventional methods are effective. \\nHowever, the challenge  remains  on developing pioneering methods, techniques, and strategies in the \\nconfrontation of the costly procedure of drug discovery. The flourishing of  deep le arning generative models \\nbrings fresh  solution s and opportunities to this field. From the generated human faces that are indistinguishable \\nwith real people22, to the text generation tools that mimic the tone and vocabulary of certain authors23, the \\nastonishing creativity of deep learning generative models brings our understanding of the machine intelligence \\nto a new level. In recen t years, the expeditions toward  genera tive chemistry mushroomed, which explore d the \\npossibility of utilizing generative models to effectively and efficiently design molecular structures with desired \\nproperties. Promising and compelling outcomes including the identification of DDR1 kinase inhib itors within \\n21 days using deep learning generative models24 may indicate that we are probably at  the corner of an upcoming \\nrevolution of drug discovery in the artificial intelligence (AI) era. This review article starts with a brief \\nevolution of AI in drug discovery , and the infrastructures in both cheminformatics and machine learning. The \\nstate-of-the-art generative models including recurrent neural networks (RNNs), variational autoencoders '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='4 \\n (VAEs), adversarial autoencoders (AAEs), and generative adversarial networks (GANs) are focused on to \\ndiscuss their fundamental architectures as well as their app lications in  the de novo  drug design.  \\n \\n2. ARTIFICIAL INTELLIGENCE  IN DRUG DISCOVERY  \\nArtificial intelligence (AI) is the study of developing and implementing techniques that enable  the machine to \\nbehave with human -like intelligence25. The conc ept of AI can be traced back to  the 1950s  when researches \\nquestioned  whether c omputers can be made to handle automate d intelligence tasks which are commonly \\nfulfilled by humans26. Thus, AI is a broad area of research that includes both (1) methodologies employing  \\nlearning processes  and (2) approaches that no learning process is involved  in. At the early stage, researchers \\nbelie ved that by defining  a sufficient number  of explicit rules to maneuver knowledge, the human -level AI can \\nbe expected  (Fig. 1a ). In the face of a specific problem, the human studying process on existing observations \\ncan contribute to the accumulation of knowledge. Explicit rules were expected to describe knowledge. By \\nprogramming and applying these rules, the answers for future observations are anticipated. This strategy is also \\nknown as symbolic AI27. Symbolic AI is an efficient solution to logical problems, for instance, chess playing. \\nHowever, when handling problems with blurry, unclear, and distorted knowledge, such as image recognition, \\nlanguage tr anslation, and to our topic , the classification of active compounds from decoys for a therapeutic \\ntarget, symbolic AI turne d out to show limited capa bility. We may define explicit rules to guide the selection of \\ngeneral drug -like compounds, Lipinski’s rule  of five28 for example, but it is almost impossible to exhaust \\nspecified rules for guiding  the s election of agonists to cannabinoid receptor 2 or other targets29. Machine \\nlearning (ML) took over symbolic AI’s position as a novel method with the ability to learn on its own.  \\n ML allows computers to solve specific tasks by  learn ing on their own30, 31. Through  directly looking at \\nthe data , computers can summarize  the rules instead of waiting  for programmers to craft them ( Fig. 1b ). In the \\nparadigm of ML -based problem solving , data and the answers to the data are functioned as input with rules  as \\nthe outcome. The produced rules can then be applied to predict answers for future data. Statistical analysis is \\nalways associated with ML, while they can be distinguished at several aspects32. The application of ML is \\nusually towards large and complex datasets, such as a datase t with millions of small molecules that cover  a huge \\nchemical space with diversified scaffolds, which statistical analysis can be incapable to deal with. The flourish \\nof ML starts in the 1990s33. The method rapidly became a dominant player in the field of AI. Commonly used  \\nML systems  in drug discovery  can be categorized into supervised learning, unsupervised learning, and \\nreinforcement learning  (Fig. 1 c). In supervised learning, the algorithms are fed with both the data and the \\nanswers to these data (label). Protein family subtypes selectivity prediction is an example for classification: the \\nclassifier is trained with numbers of sample molecules  along with their labels (the specific protein family \\nmember they interact with), and the well -trained classifier should be able to classify the future molecules20, 29, 34, \\n35. Quantitative structure -activity relationship analysis is an example for regression: the regress or is trained with '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 4}, page_content='5 \\n molecules sharing similar scaffold along with their biological activity data ( Ki, IC50, and EC 50 values for \\nexample), and the well -trained regressor should be able to predict the numeric activity value s for future \\nmolecules with the similar scaffold10, 36. In unsupervised learning, the algorithms  are trained with unlabeled data . \\nFor instance, a high -throughput screening campaign may preselect a smaller representative compound set from \\na large compound database using the clustering method to group molecu les with similar structures into \\nclusters37, 38. A subset of molecules selected from different clusters can then offer improved  structural diversity \\nto cover a bigger chemical space than random pickup. In reinforcement  learning , the learning system can choose \\nactions acco rding to its observation of the environment, and get a penalty (or reward) in return39. To achieve the \\nlowest penal ty (or highest reward), the system must learn and choose the best strategy by itself.  \\n \\n \\nFigure 1. From artificial i ntelligence to deep learning. a . The programmin g paradigm for symbolic AI. b. T he \\nprogramming paradigm for ML. c . The relationship among  artificial intelligence, machine learning, and deep \\nlearning.  \\n \\n Deep learning (DL) is a speci fic subfield of ML that adapts  neural networks to emphasize the learning \\nprocesses with successive layers  (Fig. 1 c). DL methods can transfer the representation at on e level to a higher \\nand more abstract level40. The feature of representation learning enables DL methods to discover \\nrepresentations from the raw input data for tasks such as detection and classification.  The word “deep” in DL \\nreflects this character of successive layers of representations, and  the number of layers determines  the depth of a '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='6 \\n DL model41. In contrast, conventional ML methods that transform the input data into one or two successive \\nrepresentation spaces are sometimes referred to as shallow learning methods. The vas t development in the past \\ndecades brought DL great flexibility on the selection of architectures , such as the fully connected artificial \\nneural network (ANN) or multi -layer perceptron (MLP)42, convolutional neural network (CNN)43, and recurrent \\nneural network (RNN)44. The rise of g enerative chemistry is largely benefited from the  extensive advancement \\nof generative modeling, which predominantly depends on the flourishing of DL architectures. The successful \\napplication of  the Long Short -Term Memory (LSTM) model45, a special type of RNN model, on text generation \\ninspired the simplified molecular -input line -entry system (SMILES) -based compound  design. And the \\npromi sing exercise of using  the Generative Adversarial Network (GAN) model46 for image generation \\nmotivated the fingerprint  and graph  centered molecular structural scratch. The major reason for D L to bloom \\nrapidly can be that the very method provides solution s to previously unsolvable problem s and outperforms the \\ncompetitors with simplified representation learning  process26, 40. It is foreseen that the process of molecule \\ndesign can evolve into a more efficient and effective manner with the proper fusion with DL.  \\n \\n3. DATA SOURCES  AND  MACHINE LEARNING INFRAST RUCTURES  \\nDeep learning campaigns start with high-quality input data . The successful development of generative \\nchemistry models relies on cheminformatics and bioinformatics data for the molecules and biological systems. \\nTable 1  exhib its some routinely  used databases in drug discovery for both small and large biological molecules. \\nIn a typical case of structure -based drug discovery, a 3D model of the protein (or DNA/RNA) target is critical \\nfor the following steps  on evaluating potentia l receptor -ligand interactions. PDB database47 is a good source for \\naccessing structural information for large biological systems, and the UniProt database48 will be a convenient \\nsource for sequence data. Regarding chemicals, PubChem49 can be a go-to place. PubChem is comprehensive. I t \\ncurrently contains ~103 million compounds (with unique chemical structures ) and ~253 million substances \\n(information about chemical entities). If the major focus is on bioactive molecules, ChEMBL50 can be an \\nefficient database to interact. ChEMBL currently documents ~2 million reported drug -like compounds with \\nbioactivity data  for 13,000 targets . Supposing that the interes t is more on studying the existing drugs on the \\nmarket instead of drug -like compounds, the DrugBank51 can serve. To date, DrugBank records ~ 14,000 drugs, \\nincluding ap proved small molecule drugs and biologics, nutraceuticals , and discovery -phase drugs. With virtual \\nscreening campaigns, adding some commercially available compounds to in -house libraries are preferred as \\nthey may further increase the structural diversity a nd expand the coverage of the chemical space. Once potential \\nhits were predicted to be among these compounds, the commercial availability gives them easy access for future \\nexperimental validations. Zinc database52 now archives ~230 million purchasable compounds in ready -to-dock \\nformat. It is worth mentioning that constructing topic -specific and tar get-specific databases is trending. ASD53 \\nis one example that files allosteric modulators and related macromolecu les to facilitate the research  on allosteric '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 6}, page_content='7 \\n modulation. The rising of Chemogenomics databases54, 55 for certain diseases and dru ggable targets is another \\nexample that these libraries focus on particular  areas of research .  \\n \\nWith the input data ready, the next consideration is transforming the data into machine -readable format. \\nTable 2  lists commonly used molecular representations. SMILES56 describes molecular structures in a text -\\nbased format using short ASCI I strings. Multiple SMILES strings can be generated for the same molecule with \\ndifferent starting atoms. This ambiguity  led to the effects of canonicalization  that determines which of all \\npossible SMILES will be used as the reference SMILES for a molecule.  Popular cheminformatics packages \\nsuch as OpenEye57 and RDKit58 are possible solutions for standardizing the canonical SMILES59. The canonical \\nSMILES is a type of well -liked molecular representation in generative chemistry models as it packs well with \\nlanguage processing and sequence generation techniques like RNNs. Usually the SMILES strings are fi rst \\nconverted with one -hot encoding. The categorical distribution for each element can then be produced by the \\ngenerative models. Fingerprints are another vital group of molecular representations. Molecular Access System \\n(MACCS) fingerprint60 has 166 binary keys, each of which indicates the presence of one of the 166 MDL \\nMACCS structural keys calculated from the molecular graph.  Fingerprints can be calculated th rough different \\napproaches. By  enumerating circular fragments, linear fragments, and tree fragments from the molecular graph, \\nCircular61, Path, and Tree fingerprints62 can be created.  Using fingerprints as representations may suffer from \\ninconvertibility in that the complete structure of a molecule cannot be reconstructed directly from the \\nfingerprints63. To have fingerprints calculated for a large enough compounds library to fu nction as a look -up \\nindex may be a compromised solution64. Despite thi s difficulty, fingerprints are popular among ML \\nclassification models for tasks like distinguishing active compounds from inactive ones for a given target.  \\n '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='8 \\n Table 1. Well established cheminformatics databases available for drug discovery  \\nDatabase  Descrip tion Web linkage  Examples of usage  \\nUniProt48 The Universal Protein Resource \\n(UniProt) is a resource for protein \\nsequence and annotation data  https://www.unip\\nrot.org  Protein sequence homology search, \\nalignment, and protein ID retrieving \\nespecially for structural -based drug \\ndiscov ery \\nRCSB PDB47 The Protein Data Bank (PDB) provides \\naccess to 3D structure data for large \\nbiological molecules, including \\nprotein, DNA, and RNA.  https://www.rcsb\\n.org Protein 3D structures are fundamental for \\nhot spot identification, docking simulation, \\nand molecular dynamics simulation in \\nstructural -based drug discovery.  \\nPDBbind65 PDBbind provides a collection of the \\nexperimentally measured binding \\naffinity data for all types of \\nbiomolecular complexes deposited in \\nthe PDB.  http://www.pdbb\\nind.org.cn  The receptor -ligand binding data for \\nresolved protein structures can functi on as \\nthe benchmark to evaluate future \\nsimulations  \\nPubChem49 PubChem is the world’s largest \\ncollection of chemical information.  https://pubchem.\\nncbi.nlm.nih.gov  To acquire comprehensive chemical \\ninfor mation ranging from NMR spectra, \\nphysical -chemical properties, to \\nbiomolecular interactions.  \\nChEMBL50 ChEMBL is a manually curated \\ndatabase of bioactive molecules with \\ndrug-like p roperties.  https://www.ebi.\\nac.uk/chembl/  To collect cheminformatics data of \\nreported molecules for a given target. A \\nhigh-quality compound collection is the \\nkey to the ligand -based drug discovery  \\nSureChEMBL66 SureChEMBL is a resource containing \\ncompounds e xtracted from patent \\nliterature . https://www.sure\\nchembl.org/searc\\nh/ Compound -patent associations  \\nBindingDB67 BindingDB  is a database  of measured \\nbinding affinities  for the interactions of \\nprotein considered to be drug -targets \\nwith small, drug -like molecules.  https://www.bind\\ningdb.org/bind/in\\ndex.jsp  To retrieve compound sets for a specific \\ntarget similar to ChEMBL but with  the \\nfocus on experimental binding affinities.  \\nDrugBank51 The DrugBank database combines \\ndetailed drug data with comprehensive \\ndrug target information  https://www.dru g\\nbank.ca  Drug repurposing study for existing drugs. \\nOn-target and off -target analysis for a \\ncompound.  \\nZINC52 Zinc is a database of commercially -\\navailable compounds  https://zinc.docki\\nng.org  Zinc database is good for virtual screening \\non hit identification as the compounds are \\ncommercially available for quick \\nbiological vali dations afterward s. \\nEnam ine Enamine provides an enumerated \\ndatabase of synthetically feasible \\nmolecules  https://enamine.n\\net The establishment of a target -specific \\ncompound library. Fragment -based drug \\ndiscovery.  \\nASD53 Allosteric Database (ASD)  provides a \\nresource for structure, function, disease \\nand related annotation for allosteric \\nmacromolecules and allosteric \\nmodulators  http://mdl.shsmu.\\nedu.cn/ASD/  To facilitate the research on allosteric \\nmodulation with enriched chemical data on \\nallosteric modulators.  \\nGDB68 GDB databases provide  multiple \\nsubsets of combinatorially generated \\ncompounds following chemical \\nstability and synthetic feasibility rules  http://gdb.unibe.\\nch/downloads/  Using combinator ial chemistry is a good \\nway to largely expand the chemical space.  \\n  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='9 \\n Table 2. Examples of c ommonly used molecular representations  \\nRepresentation  Description  \\nSMILES56 The simplified molecular -input line -entry system  (SMILES) is a specification in the \\nform of a  line notation  for describing the structure of  chemical species  using \\nshort ASCII  strings.  \\nCanonical SMILES  Canonicalization is a way to determine which of all possible SMILES will be used \\nas the reference SMILES for a molecular graph.  \\nInChI69 The International Chemical Identifier  (InChI) is a textual  identifier for  chemical \\nsubstances, designed to provide a standard way to encode molecu lar information.  \\nInChI Key  The condensed, 27 character  InChI Key  is a hashed  version of the full InChI.  \\nFingerprints  MACCS \\nKeys60 MACCS keys are 166 bit  structural key  descripto rs in which each bit is associated \\nwith a S MART S pattern.  \\nCircular61, 70  Circular fingerprints are created by  exhaustively  enumerating  all circular fragments \\ngrown radially from each heavy atom of the molecule up to the given radius.  \\nPath62  Path fingerprints are created by  exhaustively  enumerating  all linear fragments of a \\nmolecular graph up to a given size . \\nTree62  Tree fingerprints are generated by  exhaustively  enumerating  all tree fragments of a \\nmolecular graph up to a given size . \\nAtom Pair71 Atom  Pair fingerprints encode each atom as a type, enumerates all  \\ndistances between pairs, and then hashes the results.  \\n \\nAfter collecting the high -quality data and transforming the data into the appropriate format, it is time to \\napply data science to the development of the predictive  models. Table 3  illustrates examples of frequently \\nconsidered cheminformatics toolkits and machine learning packages. RDKit, Open Babel72, and CDK73 are \\ncheminformatics toolkits that  are comprised of a set of libraries with source codes for various functions , such as \\nchemical files I/O formatting, substructure and pattern search, and molecular representations g eneration . The \\ntypical appli cations of deploying these toolkits can contribute to virtual screening, structural similarity search, \\nstructure -activity relationship analysis, etc74. The w orkflo w environment is not unique t o the cheminformatics \\nresearch , but can facilitate the automation of data processing with a user-friendly interface. The workflow \\nsystems like KNIME75, 76 can execute tasks in succession and perform recurring tasks efficiently, such as \\niterative fingerprints calc ulation for a compound library.  The strategy of integrating cheminformatics toolkits as \\nnodes into a workflow and connecting them with edges is gaining  popularity and is increasingly employed77-79. \\nWhen it comes to ML and DL modeling, TensorFlow80, CNTK81, Theano82, and PyTorch83 are well -recognized \\npackages for employment. These packages handle low -level operations inc luding tensor manipulation and \\ndifferentiation. In contrast , Keras84 is a model -level library that deals with tasks in a modular w ay. As a high -\\nlevel API, Keras is running on top of TensorFlow, CNTK, and Theano. Scikit -Learn85 is an efficient and \\nstraightforward tool for predictive data analysis. It is known more for its role in conventio nal ML modeling as '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='10 \\n the library comprehensively integrates algorithms like Support Vector Machine ( SVM ), Random Forest  (RF) , \\nLogistic regression, Naïve Bayes  (NB) , etc.  \\n \\nTable 3 . Commonly used cheminformatics and machine learning packages  \\nPackage  Descripti on Web linkage  \\nRDKit58 RDKit is an open -source toolkit for ch eminformatics. Features \\ninclude  2D and 3D molecular operations, descriptor generation, \\nmolecular database  cartridge, etc.  https://www.rdkit.org  \\nOpen Babel72 Open Babel is an open chemical toolbox to search, convert, \\nanalyze, or store data from molecular modeling, chemistry, solid -\\nstate materials, biochemistry, or related areas.  http://openbabel.org/wik\\ni/Main_Page  \\nCDK73 The Chemistry Development Kit (CDK)  is a collection of modular \\nJava libraries for processing cheminformatic s. https://cdk.github.io  \\nKNIME75 KNIM E is a workflow environment in data science that can be \\nintegrated to automate certain cheminformatics operations.  https://www.knime.com  \\nTensorFlow80 TensorFlow is an open -source platform for machine learning. It \\nhas a  set of tools, libraries , and community resources that enable \\nresearchers to build and deploy ML applications.  https://www.tensorflow.\\norg \\nCNTK81 The Cognitive Toolkit (CNTK) is an open -source toolkit for \\ncommercial -grade distributed deep learning. It describes neural \\nnetworks as a  series of computational steps via a directed graph.  https://github.com/micro\\nsoft/CNTK  \\nTheano82 Theano is a Python library for defining, optimizing, and \\nevaluating mathematical expressions.  http://deeplearning.net/s\\noftware/theano/  \\nPyTorch83 PyTorch is an open -source machine learning library based on the \\nTorch library.  https://pytorch.org  \\nKeras84 Keras is a high -level neural networks API, written in Python and \\ncapable of running on top of  TensorFlow , CNTK , or Theano . It \\nwas developed with a focus on enabling fast experimentation.  https://keras.io  \\nScikit -Learn85 Scikit -learn is a free software machine learning library for the \\nPython programming language.  https://scikit -\\nlearn.org/stable/  \\n  \\n \\n4. GENERATIVE CHEMISTRY  WITH THE RECURRENT NEURAL NETWORK (RNN)  \\nRNN is a widely used neural network archi tecture in generative chemistry for  proposing  novel structure s. As a \\ntype of powerful generative model especially in na tural language processing, RNNs usually use sequences of \\nwords, strings , or letters as the input and output44, 86-88. In this case, the SMILES strings are usually employed as \\na molecular repr esentation. Different from ANNs and CNNs which do not have m emories, RNNs iterative ly \\nprocess sequences and store  a state holding current information. On the contrary, ANNs and CNNs process each \\ninput independently without stored information between them. When describing an RNN, it can be considered \\nas a network with an internal loop that loops over the sequence elements instead of processing in a single step \\n(Fig. 2a ). The state that stored information will be updated during each loop.  For simplicity, the process of '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='11 \\n computing the output y can follow the equation:  y = activation  (W ox + U oh + b o), where Wo and Uo are weight \\nmatrices for the input x and state h, and bo as a bias vector. Figure 2a  can represent the structure of a simple \\nRNN model. However, this  structure can suffer severely from the vanishing gradient problem which makes \\nneural networks untrainable after adding more layers. Even though the state h is supposed to hold the \\ninformation from the sequence elements previously seen, the long -term depen dencies make the learning process \\nimpossible89, 90. The Long Short -Term Memory (LSTM) algorithm45 was developed to overcome this \\nshortcoming. The LSTM layer attaches a carry track to carry information across the learning process  to counter \\nthe loss of signals f rom gradual  vanishing  (Fig. 2b ). With this carry track, the information learned from each \\nsequence element can be loaded , and the loaded information can be transported and accessed at a later stage. \\nThe process of computing the output y for LSTM is similar with the previous equation but adding  the \\ncontribution of the carry track: y = activation  (W ox + U oh + Voc + bo), where Wo, Uo, and Vo are weight \\nmatrices for the input x, state h, and carry c, and bo as a bias vector. In certain cases, multiple recurrent layers in \\na model can be stacked to e nhance representational power.  \\nA typical framework on generative modeling for molecule  generation applying LSTM algorithm ( Fig. \\n2c) starts with the collection of training molecules. The RNN model can be fine -tuned through the transfer \\nlearning that first accumulates knowledge from the large compound datasets and then produces the novel \\nstructures by learning smaller focused datasets. When  the collections of training molecules (for large sets or \\nsmall focused sets) are ready, SMILES strings can be calculate d for each molecule. One -hot encoding is a \\nregular operation for processing the molecular representations. In one -hot encoding, a unique integer index i is \\nassigned to every character in the SMILES string. Then a binary vector can be constructed of size C (the \\nnumber of unique characters in the string) with all zeros but for the ith entry which is one. For instance, there \\nare four ( C = 4) unique characters, “C”, “N”, “c”, and “1” in SMILES strings, input “C” is transferred  to (1, 0, 0, \\n0), “N” to (0, 1, 0, 0), “c” to (0, 0, 1, 0), and “1” to (0, 0, 0, 1) after one -hot encoding. In practice, usually an \\nadditional starting character like “G” and an ending character like “E” will be added to the SMILES to denote a \\ncomplete string. The neural network with LSTM l ayer(s) can be trained to predict the n+1th character given the \\ninput of string with n characters. The probability of distribution for the n+1th character is calculated as the loss \\nto evaluate the model performance. With the trained model, the sampling pro cess can start with the starting \\ncharacter or certain SMILES strings of molecular fragments to sample the next character until the ending \\ncharacter is hi t. The SMILES strings are reversed from the generated binary matrices according to the previous \\none-hot encoding to construct the molecular graphs as the output for this gen erative model.  \\n '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 11}, page_content='12 \\n \\n \\nFigure 2 . The RNN, the LSTM, and their application in generative chemistry. a. The s chematic illustration of the \\nRNN, the neural network with an internal loop. b. The schematic illustration of data processing with the LSTM. c. \\nThe typical framework on building generative models applying RNN for molecule s generation.  \\n \\nRepresentative c ase studies are discussed in this paragraph. All the case applications covered in this \\nreview are summarized in Table 4 . Anvita Gupta et al. trained an LSTM -based generative model with transfer \\nlearning to generate libraries of molecules with structural similarity to known actives f or PPAR\\uf067 and trypsin91. \\nThe model was first trained with 550,000 SMILES strings of active compo unds from ChEMBL and further \\nfine-tuned with SMILES strings for 4,367 PPAR \\uf067 ligands and 1 ,490 trypsin inhibitors. Among the valid \\ngenerated molecules, around 90% are unique from the know n ligands and are different from each other. The \\nproposed model was as sessed for fragment -based drug discovery as well. In fragmen t-based drug discovery, \\nfragment  growing is a strategy for novel compounds generation with the identified fragment lead. Substitutions \\ncan be added to the identified fragment with the consideratio n of pharmacophore features and proper physical -\\nchemical properties to enhance the receptor -ligand interactions92. Instead of using the starting character to '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='13 \\n initiate the generative process, the SMILES string of the molecular fragment can be read and extended by \\ncalculating the probability of distribution for the next character. Marwin H. S. Segler et al. also reported their \\napplication of LSTM -based generative models for structure generation with transfer learning93. There was a \\ngood correlation between the generated structures and the molecules used for training. Notably , the complete de \\nnovo  drug design cycle can be achieved with target prediction models for scoring. As the target prediction \\nmodel can be a molecu lar docking algorithm or even robot synthesis and bio -testing system, the drug design \\ncycle does not require known active compounds to start. Chemical Language Model (CLM) proposed by \\nMichael Moret et al. is another example of applying LSTM -based generativ e models to work with chemical \\nSMILES strings with transfer learning processes94. This approach enables the early stage molecular design in a \\nlow data regime. When it comes to real -world validation, Daniel Merk et al. published their prospective study \\nwith experimental evaluations95. Using the SMILES strings as the input, the LSTM -based generative model was \\ntrained and fine -tuned with the transfe r learning process for the peroxisome proliferator -activated receptor . Five \\ntop-ranked compounds designed by the model were synthesized and tested. Four of them have nanomolar to \\nlow micromolar activities  in cell -based assays. Besides using the LSTM algori thm, some other RNN \\narchitectures such as implementing Gated Recurrent Unit96 (GRU) can also have promising applications. GRU \\nlayers work with the same principle as LSTM layers but may have less representational power. Shuangjia \\nZheng et al. developed a quasi -biogenic molecular generator with GRU layers97. As biogenic compounds and \\npharmaceutical agents are biologically relevant, ov er 50% of existing drugs re sult from drug discovery \\ncampaigns starting with biogenic molecules. Their generative model is an effort to explore greater biogenic \\ndiversity space. Similarly, focused compound libraries can be constructed with transfer learning processes.  \\n \\n5. GENERATIVE  CHEMISTRY WITH THE VARIATIONAL AUTOENCODER  (VAE)  \\nThe principle aim of an autoencoder (AE) is to construct a low -dimensional latent space of compressed \\nrepresentations that each element can be reconstructed to the original input  (Fig. 3a ). The module that maps the \\noriginal input data, which is in high -dimension, to a low -dimensional representation is called t he encoder, while \\nthe module that realizes the mapping and reconstructs the original input from the low -dimensional \\nrepresentation is called the decode r41, 98. The encoder and the decoder are usually neural networks with RNN \\nand CNN architectures as SMIL ES strings and molecular graphs are commonly used molecular representations. \\nWith the molecular representations calculated, a  typical data processing procedure with AE on molecule \\ngeneration starts with encoding the input into a low -dimensional latent spac e. Within the latent space, the axis \\nof variations from the  input  can be encoded . Using the variation of molecular weight (M.W.) as an example, \\nwhile in practice the features learned can be highly abstractive as the M.W. is used here for simplified \\nillustr ation, the points along t his axis  are embedded representations of compounds with different M.W. These \\nvariations are termed concept vectors. With an identified vector, it makes the molecular editing possible by '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 13}, page_content='14 \\n exploring the representations in a relevant d irection. The encoded latent space with compressed representations \\ncan then be sampled with the decoder to map them back to m olecular representations . Novel structures \\nalongside the original input can be  expected.  \\n \\n \\nFigure 3. The autoencoder and the variational autoencoder. a . An autoencoder encodes input molecules into \\ncompressed representations and decodes them back . b. A variational autoencoder maps the molecules into the \\nparameters of a statistical distribution  as the latent space is a continuous nume rical representation.  \\n \\nThe concept of VAE was first proposed by Kingma and Welling at the end of 201399, 100. This technique \\nquickly gained  popularity in building robust generative models for images, sounds, and texts101-103. The AE \\ncompresses a molecule x into a fixed code in the continuous latent space z, and trends to summarize the explicit \\nmapping rules as the number of adjustable parameters is often much  more than the number of training \\nmolecules. These explicit rules make t he decoding of random  points in the continuous latent space challenging \\nand sometimes impossible26. Instead, VAE maps the molecules into the parameters of a statistical distribution \\n(Fig. 3b ). With p(z) describing  the distribution of prior continuous latent space, the probabilistic encoding '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='15 \\n distributi on is q\\uf06a(z|x) and the probabilistic decoding distribution is p\\uf071(x|z). The training iterations with back  \\npropagation will gradually optimize the parameters of both q\\uf06a(z|x) and p\\uf071(x|z). VAE is  fundamentally a latent \\nvariable mode l p(x,z ) = p\\uf071(x|z)p(z). The s tochasticity of the training process enables the latent space to encode \\nvalid representations, which further results in a structured latent space100. Both the reconstruction loss and the \\nregularization loss are often used for paramet er optimization during the training process. The reconstruction loss \\nevaluates whether the decoded samples match the input while the regularization loss investigates whether the \\nlatent space is overfitting to the training data.  \\n Applications of VAE for ge nerating chemical structures  started in 2016 as Rafael  Goḿez -Bombarelli et \\nal. developed a VAE -based automatic chemical design system104. In their practice, the ZINC database and QM9 \\ndataset were referred to as the sources for collecting molecules.  The QM9 dataset archives small molecules \\nfollowing three rules: (1) no  more than 9 heavy atoms, (2) with 4 distinct atomic numbers, and (3) with 4 bond \\ntypes. Canonical SMILES strings were calculated as the molecular representation. The encoder maps input \\nSMILES strings into the continuous real -valued vector s, and the decode r reconstructs  molecular representations \\nfrom the se vector s. The e ncoder was formed with three convolutional layers and one fully connected dense \\nlayer while the decoder contained three GRU layers. The architectures of CNNs and RNNs were compared for \\nstring encoding and convolutional layers achieved superior performance. The last layer of the decoder would \\nreport a probability distr ibution for characters of the S MILES string at each position. This stochastic operation \\nallowed the same point in the latent sp ace to have different decoded outcomes. Besides, they added one \\nadditional module for property prediction. An MLP was jointed to predict the property values from the \\ncontinuous representation created by the encoder in order to optimize the desired properti es for the new \\nmolecules. Thomas Blaschke et al. tested various generative AE models including VAE for compound design \\ntargeting dopamine receptor 2  (DRD2)105. Their study showed that the generated latent space preserved the \\nchemical similarity principles. The generated molecules similar to known acti ve compounds can be observed. In \\ntheir VAE model, CNN layers were used for the encoder for pattern recognition and the RNN layers of GRU \\ncells were adapted for the decoder. The ChEMBL database functioned as the data source for molecular \\nstructures. Canonic al SMILES were prepared as the molecular representation. Similarly, a n SVM classification \\nmodel trained with extensive circular fingerprint (ECFP) of active and inactive DRD2 ligands was integrated to \\ninvestigate the newly generated molecules.  Boris Sattar ov et al. combined a sequence -to-sequence V AE model \\nwith generative topographic mapping (GTM) for molecular design106. Both the encoder and the decoder were \\nRNN models c ontaining two LSTM layers in their practice. SMILES strings with one -hot encoding for \\nmolecules from  the ChEMBL database were prepared prior to the training. Their GTM module contributed to \\nthe selection  of sampling points in the VAE latent space, which fa cilitated the generation of a focused library of \\ncompounds with desired properties.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='16 \\n Besides the use of SMILES strings, molecular graphs have also been applied as a type of molecular \\nrepresentation  to feed the VA E models. Bidisha Samanta et al. proposed Ne VAE , a VAE -based  compound  \\ngenerative model employing  molecular graphs107. The molecular structures  are usually not grid -like and come \\nwith an inconsistent number of nodes and edges, which impedes the use of molecular graphs as representations. \\nIn their wo rk, the molecular graphs were prepared for  drug-like compounds  collected from the ZINC database \\nand QM9 dataset. The nodes and edges in the graph represent atoms and bonds respectively. The node features \\nare types of atoms with one -hot encoding and the edge weights are bond types (saturated bonds, unsaturated \\ndouble/tripl e bonds, etc.) . The purpose of training is to enable the VAE to create credible molecular graphs \\nincluding node features and edge weights. Another example is GraphVAE . Martin Simonovsky et al. proposed \\nGraphVAE  to facilitate the compound design using  molec ular graphs108. Their central hypothesis was to decode \\na probabilistic fully -connected graph in which the existence of nodes, edges, and their attributes are independent \\nrandom variables.  The encoder was a feed -forward network with convolutional layers  and the architecture for \\nthe decoder was an MLP. The model training and evaluation involved the molecules from the ZINC database \\nand QM9 dataset.  Some other generative applications can switc h the topic to lead optimizations with methods \\nsuch as scaffold hopping, substitutions design, and fragment -based approaches. One example is the DeLinker \\nwhich was  proposed by Fergus Imrie et al. to incorporate two fragments into a new molecule109. This method is \\nVAE -based, using molecular graphs as the i nput. The design process heavily relied  on 3D structural information \\nthat considers relative distance and orientation between the starting fragments.  \\n \\n6. GENERATIVE CHEMISTRY WITH THE ADVERSARIAL AUTOENCODER  (AAE)  \\nThe architecture of the AAE is comparativ ely similar to the VAE except the appending of the additional \\ndiscriminator network110. An AAE trains three modules, an encoder, a decoder, and a discr iminator  (Fig. 4). \\nThe encoder learns the input data and maps the molecule into the latent space following the distribution of \\nq\\uf06a(z|x). The decoder reconstructs molecules through sampling from the latent space following the probabilistic \\ndecoding distribut ion of p\\uf071(x|z). And the discriminator distinguishes the distribution of the latent space z ~ q\\uf06a(z) \\nfrom the prior distribution z’ ~ p(z). During the training iterations, the encoder is modified consistently to have \\nthe output, q\\uf06a(z|x), follow a specific di stribution, p(z), in an effort to minimize the adver sarial cost of the \\ndiscriminator. A simplistic prior, like Gaussian distribution, is assumed in VAE, while alternative priors can \\nexist in real -world practices111. The AAE  architecture  with the additional discriminator  module demonstrates \\nimproved adaptability .  \\n '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='17 \\n \\n \\nFigure 4. The illustrated a rchitecture of an adversarial autoencoder. A discriminator network is appended to \\ncalculate the adversarial cost for discriminating p(z) from q\\uf06a(z). As a result, the outcome latent space from the \\nencoder is driven to follow the prior distribution.  \\n \\nThomas Blaschke et al.  summarized a three -step training process in their compound design practice with \\nAAE: (1) The simultaneous training of  both the encoder and the decoder to curtail  the reconstruction loss  of the \\ndecoder; (2) The training of the discriminator to distinguish the distribution of the latent space, q\\uf06a(z), from the \\nprior distribution p(z) effectively; (3) The training of the enc oder to minimize the adversarial cost for \\ndiscriminating p(z) from q\\uf06a(z)105. The training iterations continue until the reconstruction loss converges. Artur \\nKadurin et al. proposed the method of using  a generative adversarial autoencoder model to identify fingerprints \\nof new molecules with potential a nticancer properties111. The input molecules come from a  small data set of \\ncompounds profiled on the MCF -7 cell line . The MACCS fingerprints were used as the molecular \\nrepresentation and two fully connected dense layers with different dimensions were used as the network \\narchitecture for the encoder, decoder, and the discriminator.  One notable modification in this study was the \\nremoval of the batch normalization layers for the discriminator. Batch normalization is an optimiza tion method \\nthat reduces the covariance shift among the hidden units and allows each layer to learn more independently. In \\nthe authors’ opinion, the noise from the generator can be masked into target random noise with the batch \\nnormalization layers, which prohibits the training of the discriminator. As each bit of the MACCS fingerprints \\nrepresents certain substructure features, the learned structural information by machine can be beneficial to the \\ndesign of chemical derivatives for identified leads. Daniil Polykovskiy et al. reported their work on building a \\nconditional AAE for molecule design targeting Janus kinase 3  (JAK3)112. The contributions from  a set of \\nphysical -chemical properties including bioactivity, solubility, and synthesizability were considered and the \\nmodel was conditioned to produce molecules with specified properties. Clean lead molecules were c ollected \\nfrom the ZINC database and encoded as SMILES strings. The LSTM layers are adapted for building the '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='18 \\n encoder and the decoder networks. Both in silico  method (molecular docking) and in vitro  assay ( inhibition of \\nJAK2 and JAK3 ) were conducted as the e valuation for the newly generated molecules. Rim Shayakhmetov et al. \\nreported a bidirectional AAE  model that generates molecules with the capacity of inducing a desired change in \\ngene expression113. The model was va lidated using LINCS L1000, a database that collects gene expression \\nprofiles. The molecular structures x and induced gene expression changes y contributed to a joint model p(x,y) . \\nIn this specific conditional task, there is no direct association between x and y as certain changes at the gene \\nexpression are irrelevant to the drug -target interactions. The proposed bidirectional AAE model then learned  the \\njoint distribution and decomposed objects into shared features, exclusive features to  x, and exclusive fea tures to \\ny. Therefore, t he discrimin ator that divides  the latent representations into sh ared and exclusive sections was  \\nconstructed to secure the conditional generation to be consequential.  \\n \\nTable 4 . Representative applications of generative chemistry cov ered in this review   \\n# Generative \\narchitecture  Neural networks \\ninvolved  Data \\nsource  Molecular \\nrepresentation  Note  Ref. \\n1 RNN  LSTM  ChEMBL  SMILES  The application was extended to \\nfragment -based drug design . 91 \\n2 RNN  LSTM  ChEMBL  SMILES  The d esign -synthesis -test cycle was \\nsimulated with tar get prediction models \\nfor scoring.  93 \\n3 RNN  LSTM  ChEMBL  SMILES  A chemical language model (CLM) in low \\ndata regimes.  94 \\n4 RNN  LSTM  ChEMBL  SMILES  A prospective application with \\nexperimental validations of top -ranking \\ncompounds.  95 \\n5 RNN  GRU  ZINC  \\nChEMBL  SMILES  The generative model explored greater \\nbiogenic diversity space.  97 \\n6 VAE  Encoder: CNN  \\nDecoder: GRU  ZINC \\nQM9  SMILES  An MLP model was jointed to predict \\nproperty values.  104 \\n7 VAE  Encoder: CNN  \\nDecoder: GRU  ChEMBL  SMILES  An SVM classification model was added \\nto evaluate the outco me. 105 \\n8 VAE  Encoder: LSTM  \\nDecoder: LSTM  ChEMBL  SMILES  A sequence -to-sequence VAE model was \\ncombined with generative topographic \\nmapping (GTM) for molecular design.  106 \\n9 VAE  Encoder: CNN  \\nDecoder: CNN  ZINC  \\nQM9  Molecular \\ngraph  The nodes and edges in the graph of \\nNeVAE represent atoms and bonds \\nrespectively.  107 \\n10 VAE  Encoder: CNN  \\nDecoder: MLP  ZINC  \\nQM9  Molecular \\ngraph  The central hypothesis of GraphVAE was \\nto decode a probabilistic fully -connected \\ngraph . 108 '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='19 \\n 11 VAE  Encoder: GGNN# \\nDecoder: GGNN  ZINC  \\nCASF* Molecular \\ngraph  DeLinker was designed to incorporate two \\nfragments into a new molecule.  109 \\n12 AAE  Encoder: MLP  \\nDecoder: MLP  \\nDiscriminator: MLP  MCF -7^ MACCS \\nfingerprints  Fingerprints cannot be directly converted \\nto structures but can provide certain \\nsubstruct ure information.  111 \\n13 AAE  Encoder: LSTM  \\nDecoder: LSTM  \\nDiscriminator: MLP  ZINC  SMILES  The generated molecules targeting JAK3 \\nwere evaluated with in silico  and in vitro  \\nmethods.  112 \\n14 AAE  Encoder: GRU  \\nDecoder: GRU  \\nDiscriminator: MLP  LINCS& \\nChEMBL  SMILES  The combination of molecules and gene \\nexpression data were analyzed.  113 \\n15 GAN  Discriminator: CNN  \\nGenerator: LSTM  ZINC  SMILES  Sequence generation with objective -\\nreinforced generative adversarial networks \\n(ORGAN) . 114 \\n16 GAN  Discriminator: MLP  \\nGenerator: MLP  ZINC  Molecular \\ngraph  The model operated in the latent space \\ntrained by the Ju nction Tree VAE . 115 \\n17 GAN  Discriminator: MLP  \\nGenerator: MLP  LINCS& SMILES  The compound design was connected to \\nthe systems biology.  116 \\n18 GAN  Encoder: LSTM  \\nDecoder: LSTM  \\nDiscriminator: MLP  \\nGenerator: MLP  ChEMBL  SMILES  The concept of the autoencoder and the \\ngenerative adversarial network was \\ncombined to propose a latentGAN.  117 \\n#GGNN represent s the gated graph neural network. *CASF is also known as PDBbind core set.  ^MCF -7 represents a small \\ndata set of compounds profiled on the MCF -7 cell line . &LINCS represents the LINCS L1000 dataset that collects gene \\nexpression profiles.  \\n \\n7. GENERATIVE CHEMISTRY WITH THE GENERATIVE ADVERSARIAL NETWORK (GAN)  \\nThe architecture of the convolutional neural netwo rk43 (CNN) is  briefly covered in this section as the \\nconvolutional layers are widely used i n GAN mod eling. The implementation  of convolutional layers can also be \\nfound  in case studies discussed above among autoencoder models . A con volutional layer does not learn  an input  \\nglobally  but focus es on the local pattern within a receptive field , the kernel  (Fig. 5a). The low -level patterns \\nlearned in a prior layer can then be concentrated on the high-level features at the subsequent layers118, 119. This \\ncharacteristic allows the CNN to learn and summarize  abstract patterns with complexity. Another characteristic  \\nthat comes out from the  local pattern learning is that the learned features can be recognized anywhere118. It \\nenables the CNN to process input data with  efficiency  and powerful ness even with a  smaller number of input \\nsample  representations. Meanwhile, multiple feature maps (filters) can be stacked to encode diffe rent a spects of \\nthe input data. Applying several  filters capacitates  a CNN model to detect distinct  features anywhere  among the \\ninput data. The pooling operation on the other hand subsamples the feature map  to reduce the number of \\nparameters and eventually, the computational load120. Using a max -pooling layer as one example, only the max \\ninput value in that pooling kernel will be k ept. Alongside with dropout layers and regularization p enalties, the '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 19}, page_content='20 \\n pooling layers also contribute  to confronting the overfitting issues. Putting together, the convolutional layers, \\npooling layers, and dense layers are carefully selected and arranged to construct a sophisticated CNN \\narchitecture.  \\n \\n \\nFigure 5. Sample architecture  of the convolutional neural network and the framework  of a generative adversarial \\nnetwork . a. The careful selection and arrangement of convolutional layers, pooling layers, and dense layers , etc.  \\nconstitute a convolutional neural netw ork. b . The generative adversarial network comprises  two modules, the \\ngenerator and the discriminator.  Both the generative loss and discriminative  loss are monitored during the \\ntraining process.  \\n \\n The concept of the GAN was first raised  by Ian Goodfellow  in 201446. The method quickly gained \\npopularity on generative tasks regarding image, video, and audio processing and related areas121-123. Two \\nmodels, the discriminato r and the generator are trained  iteratively and simultaneously during the adver sarial \\ntraining process63. The discriminator is supposed to discover the hidden patterns behind the input data and to \\nmake accurate discrimination of the authentic data from the ones generated by the generator. The generator is \\ntrained to keep proposing compelling data t o fool the well -trained discriminator by consistently optimizing the \\ndata sampling process. The training process is a zero -sum noncooperative game with the purpose of achieving '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='21 \\n the Nash equilibrium by the discriminator and the generator. In generative chem istry, the g enerator generates \\nSMILES strings, molecular graphs, or fingerprints, depending  on the selection of the molecular representation, \\nusing the latent random inputs ( Fig. 5b ). The generated molecules are mixed with the samples of real \\ncompounds to feed the discriminator after correct labeling. The discriminative loss is calculated to evaluate \\nwhether the discriminator can distinguish the real compounds from the generated ones, while the generative loss \\nis computed to assess whether the generator can  fool the discriminator by generating undistinguishable \\nmolecules. The constringency of both loss functions after the iterative training indicates that even a well -\\nestablished discriminator can be misled to classify generated molecules as real, which furth er reflects that the \\ngenerator has learned and accumulated authentic data patterns to create captivating compounds. However, it is \\nworth mentioning that the simultaneous optimization of both loss functions is challenging as the instability can \\nlead to the gradient of one part instead of both being favored (results in a stronger discriminator or generator, \\nbut not both).  Another limitation may come from the restricted chemical space that is being covered by the \\ngenerated molecules64. To confront the discriminator and minimize the generative  loss, the generator can only \\nexplore a limited chemical space defined by the real compounds.  \\n Gabriel Guimaraes et al. presented  a seq uence -based GAN framework termed objective -reinforced \\ngenerative adversarial network (ORGAN)114 that includes domain -specific objectives to the training process \\nbesides the discriminator reward. The discr iminator drove the generated samples to follow the distribution of \\nthe real data and the domain -specific objectives secured that the traits maximizing the specific heuristic would \\nbe selected. The d rug-like and nondrug -like molecules were collected from ZI NC databases. SMILES st rings \\nwere calculated  as the molecular  representations. A CNN model was designed as the discriminator to classify \\ntexts, and a n RNN model with LSTM units was used as the generator. Łukasz Maziarka et al. introduced Mol -\\nCycleGAN for derivatives design and compound optimization115. The model could generate structures with high \\nsimilarity to the original input but improved values on considered properties. Molecular graphs of compounds \\nextracted from  the ZINC database were used as the molecular representation. The model operated in the latent \\nspace trained by the Junction Tree VAE. Dense layers and fully connected residual layers constituted the \\ngenerator and the discriminator. Oscar Méndez -Lucio et al.  reported a GAN model to connect  the compound \\ndesign with systems biology116. They have shown that active -like molecules can be generated given that the \\ngene expression signature of the selected target is supplied . The architectures  of both the discriminator  and the \\ngenerator were composed with dense layers. There were two stages of training: in stage I, the random noise was \\ntaken as the input, while in stage II, the output from stage I and the gene expression signature were taken.  \\nOleksii Prykhodko et al. combined the concept of AE with G AN and proposed a latent vector -based GAN \\nmodel117. A heteroencoder mapped one -hot encoded SMILES strings into the latent space and the generator and  \\ndiscriminator would directly use the latent vector to focus on the optimization of the sampling process. A pre -'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='22 \\n trained heteroencoder was then used to transfer the generated vectors back to molecular structures.  Both general \\ndrug-like compounds and target -biased molecules were generated as applications of the method.  \\n \\n8. CONCLUSION AND FUTURE PERSPECTIVES  \\nBesides the successful generative chemistry stories described above, challenges and opportunities can be found \\nat the following four aspects:  (1) the syn thetic feasibility of the generated structures, (2) the alternative \\nmolecular representations that can better portray a structure, (3) the generation of macro -molecules, and ( 4) the \\nclose -loop automation in combination with experimental validations. Wenhao  Gao et al. pointed out that \\ngenerative models can propose unrealistic molecules even with high performance scores on quantitative \\nbenchmarks124. Some existing methods of evaluating the synthesizability are based on synthetic routes and \\nmolecular structural data, which require heuristic definition to be complex and comprehensive125, while the \\nchange of on e single functional group to a scaffold can cause a  distinctive synthetic pathway.  The ignorance of \\nsynthesizabilit y turns out to be an eminent hindrance of connecting generative models with medicinal chemistry \\nsynthesis. The molecular representations such as SMILES st rings and molecular fingerprints serve well on \\ndescribing small molecules at the current stage. Howeve r, it will be appealing if the novel representations can \\nbe designed to also consider three -dimensional geometry data. Chiral compounds may exhibit divergent \\nactivities to the biological system126, and even th e conformational change of the same small molecule can alter \\nthe receptor -ligand interactions. The case studies that deployed molecular graphs as the representation illustrate \\nthe benefits of working with structures directly107-109, 115. The extended consideration of bond type, length, and \\nangles improves  the performance of feature extraction on spatial patter ns. Peptides possess superior advantage \\namong protein subtype selectivity. The strategy of developing antibodies and peptides as therapeutic agents \\ndraw s increasing attention from both the academia and industry. Deep learning is  data-driven research. Curre nt \\ngenerative chemistry applications mainly focus on the design o f small molecules as there is  increased \\navailability of accessing chemical data127. As the construction of protein -related databases is rising, the attempts \\nof de novo protein generation are expected128. Better representations are certainly required for describing \\nprotein , as the folding and its conformation are even more critical to determine the functionality. Lastly, it is \\nnoteworthy of how to integrate the generative chemistry into  the drug design framework to close the loop of this \\nautomated process. Marwin H. S. Segler  et al. mentioned a design -synthesis -test cycle in their application of \\nusing the RNN model to generate molecules93. Ideally, the HTS will first recognize some hit compounds for a \\ngiven target. The identified hits will contribute to the iterative training of a deep learning generative model for \\nnovel compounds generation,  and a machine learning -based target prediction model for virtual classification. \\nThe top molecules will be synthesized and tested with biological assays. The true new actives can then be \\nappended to the identified hits, which closes the loop.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='23 \\n  In a nutshe ll, this paper reviewed  the latest advances of generative chemistry that utilizes deep learning \\ngenerative models to expedite the drug discovery process. The review starts with a brief history of AI in drug \\ndiscovery to outline this emerging paradigm. Comm only used chemical databases, molecular representations, \\nand operating sources of cheminformatics and machine learning are covered as the infrastructure. The detailed \\ndiscussion s on RNN, VAE, AAE, and GAN are centered, which is followed by future perspecti ves. As a fast -\\ngrowing area of research, we are optimistic to expect a boosting number of studies on generative chemistry. We \\nare probably at the corner of an upcoming revolution of drug discovery in the AI era, and the good news is that \\nwe are witnessing the change.  \\n \\n9. AUTHOR INFORMATION  \\nCorresponding author  \\nAuthor to whom correspondence should be addressed: Xiang -Qun Xie  \\nNotes  \\nThe authors declare no competing financial interest.  \\n \\n10. ACKNOWLEDGEMENTS  \\nAuthors would like to acknowledge the funding support to the Xie laboratory from the NIH NIDA (P30 \\nDA035778A1) and DOD (W81XWH -16-1-0490).  \\n \\n11. REFERENCES  \\n1. Chan, H. S.; Shan, H.; Dahoun, T.; Vogel, H.; Yuan, S., Advancing drug discovery via artificial intelligence. \\nTrends in pharmacologi cal sciences 2019 . \\n2. Dickson, M.; Gagnon, J. P., The cost of new drug discovery and development. Discovery medicine 2009 , \\n4, 172 -179.  \\n3. Chen, H.; Engkvist, O.; Wang, Y.; Olivecrona, M.; Blaschke, T., The rise of deep learning in drug \\ndiscovery. Drug disc overy today 2018 , 23, 1241 -1250.  \\n4. Broach, J. R.; Thorner, J., High -throughput screening for drug discovery. Nature 1996 , 384, 14 -16. \\n5. Kroemer, R. T., Structure -based drug design: docking and scoring. Current protein and peptide science \\n2007 , 8, 312 -328. \\n6. Blundell, T. L., Structure -based drug design. Nature 1996 , 384, 23.  \\n7. Bacilieri, M.; Moro, S., Ligand -based drug design methodologies in drug discovery process: an overview. \\nCurrent drug discovery technologies 2006 , 3, 155 -165.  \\n8. Pagadala, N. S.; Sy ed, K.; Tuszynski, J., Software for molecular docking: a review. Biophysical reviews \\n2017 , 9, 91 -102.  \\n9. Bian, Y. -m.; He, X. -b.; Jing, Y. -k.; Wang, L. -r.; Wang, J. -m.; Xie, X. -Q., Computational systems \\npharmacology analysis of cannabidiol: a combination of  chemogenomics -knowledgebase network analysis and \\nintegrated in silico modeling and simulation. Acta Pharmacologica Sinica 2019 , 40, 374 -386.  \\n10. Bian, Y.; Feng, Z.; Yang, P.; Xie, X. -Q., Integrated in silico fragment -based drug design: case study with \\nallosteric modulators on metabotropic glutamate receptor 5. The AAPS journal 2017 , 19, 1235 -1248.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='24 \\n 11. Wang, J.; Wolf, R. M.; Caldwell, J. W.; Kollman, P. A.; Case, D. A., Development and testing of a general \\namber force field. Journal of computational chemist ry 2004 , 25, 1157 -1174.  \\n12. Vanommeslaeghe, K.; Hatcher, E.; Acharya, C.; Kundu, S.; Zhong, S.; Shim, J.; Darian, E.; Guvench, O.; \\nLopes, P.; Vorobyov, I., CHARMM general force field: A force field for drug ‐like molecules compatible with \\nthe CHARMM all ‐atom additive biological force fields. Journal of computational chemistry 2010 , 31, 671 -690.  \\n13. Ge, H.; Bian, Y.; He, X .; Xie, X. -Q.; Wang, J., Significantly different effects of tetrahydroberberrubine \\nenantiomers on dopamine D1/D2 receptors revealed by experimental study and integrated in silico simulation. \\nJournal of computer -aided molecular design 2019 , 33, 447 -459.  \\n14. Hajduk, P. J.; Greer, J., A decade of fragment -based drug design: strategic advances and lessons learned. \\nNature reviews Drug discovery 2007 , 6, 211 -219.  \\n15. Yang, S. -Y., Pharmacophore modeling and applications in drug discovery: challenges and recent \\nadvances. Drug discovery today 2010 , 15, 444 -450.  \\n16. Wieder, M.; Garon, A.; Perricone, U.; Boresch, S.; Seidel, T.; Almerico, A. M.; Langer, T., Common hits \\napproach: combining pharmacophore modeling and molecular dynamics simulations. Journal of chemical \\ninformation and modeling 2017 , 57, 365 -385.  \\n17. Liu, Z.; Chen, H.; Wang, P.; Li, Y.; Wold, E. A.; Leonard, P. G.; Joseph, S.; Brasier, A. R.; Tian, B.; Zhou, J., \\nDiscovery of Orally Bioavailable Chromone Derivatives as Potent and Selective BRD4 Inhibitors: S caffolding \\nHopping, Optimization and Pharmacological Evaluation. Journal of Medicinal Chemistry 2020 . \\n18. Hu, Y.; Stumpfe, D.; Bajorath, J. r., Recent advances in scaffold hopping: miniperspective. Journal of \\nmedicinal chemistry 2017 , 60, 1238 -1246.  \\n19. Muegge, I.; Mukherjee, P., An overview of molecular fingerprint similarity search in virtual screening. \\nExpert opinion on drug discovery 2016 , 11, 137 -148.  \\n20. Fan, Y.; Zhang, Y.; Hua, Y.; Wang, Y.; Zhu, L.; Zhao, J.; Yang, Y.; Chen, X.; Lu, S.; Lu, T., Inve stigation of \\nMachine Intelligence in Compound Cell Activity Classification. Molecular Pharmaceutics 2019 , 16, 4472 -4484.  \\n21. Minerali, E.; Foil, D. H.; Zorn, K. M.; Lane, T. R.; Ekins, S., Comparing Machine Learning Algorithms for \\nPredicting Drug -Induced L iver Injury (DILI). Molecular Pharmaceutics 2020 . \\n22. Karras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; Aila, T., Analyzing and improving the image \\nquality of stylegan. arXiv preprint arXiv:1912.04958 2019 . \\n23. Wen, T. -H.; Gasic, M.; Mrksic, N.; Su, P. -H.; Vandyke, D.; Young, S., Semantically conditioned lstm -based \\nnatural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745 2015 . \\n24. Zhavoronkov, A.; Ivanenkov, Y. A.; Aliper, A.; Veselov, M. S.; Aladinskiy, V. A.; Aladinskaya, A. V.; \\nTerentiev, V. A.; Polykovskiy, D. A.; Kuznetsov, M. D.; Asadulaev, A., Deep learning enables rapid identification \\nof potent DDR1 kinase inhibitors. Nature biotechnology 2019 , 37, 1038 -1040.  \\n25. Turing, A. M. Computing machinery and inte lligence. In Parsing the Turing Test ; Springer: 2009, pp 23 -\\n65. \\n26. Chollet, F., Deep Learning mit Python und Keras: Das Praxis -Handbuch vom Entwickler der Keras -\\nBibliothek . MITP -Verlags GmbH & Co. KG: 2018.  \\n27. Segler, M. H.; Preuss, M.; Waller, M. P., Pl anning chemical syntheses with deep neural networks and \\nsymbolic AI. Nature 2018 , 555, 604 -610.  \\n28. Lipinski, C. A., Rule of five in 2015 and beyond: Target and ligand structural limitations, ligand \\nchemistry structure and drug discovery project decisions.  Advanced drug delivery reviews 2016 , 101, 34 -41. \\n29. Bian, Y.; Jing, Y.; Wang, L.; Ma, S.; Jun, J. J.; Xie, X. -Q., Prediction of orthosteric and allosteric \\nregulations on cannabinoid receptors using supervised machine learning classifiers. Molecular pharm aceutics \\n2019 , 16, 2605 -2615.  \\n30. Lo, Y. -C.; Rensi, S. E.; Torng, W.; Altman, R. B., Machine learning in chemoinformatics and drug \\ndiscovery. Drug discovery today 2018 , 23, 1538 -1546.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='25 \\n 31. Jing, Y.; Bian, Y.; Hu, Z.; Wang, L.; Xie, X. -Q. S., Deep learning f or drug design: an artificial intelligence \\nparadigm for drug discovery in the big data era. The AAPS journal 2018 , 20, 58.  \\n32. Bzdok, D.; Altman, N.; Krzywinski, M., In; Nature Publishing Group: 2018.  \\n33. Vamathevan, J.; Clark, D.; Czodrowski, P.; Dunham, I.; Ferran, E.; Lee, G.; Li, B.; Madabhushi, A.; Shah, \\nP.; Spitzer, M., Applications of machine learning in drug discovery and development. Nature Reviews Drug \\nDiscovery 2019 , 18, 463 -477.  \\n34. Korotcov, A.; Tkachenko, V.; Russo, D. P.; Ekins, S., Compariso n of deep learning with multiple machine \\nlearning methods and metrics using diverse drug discovery data sets. Molecular pharmaceutics 2017 , 14, \\n4462 -4475.  \\n35. Ma, X. H.; Jia, J.; Zhu, F.; Xue, Y.; Li, Z. R.; Chen, Y. Z., Comparative analysis of machine lea rning \\nmethods in ligand -based virtual screening of large compound libraries. Combinatorial chemistry & high \\nthroughput screening 2009 , 12, 344 -357.  \\n36. Verma, J.; Khedkar, V. M.; Coutinho, E. C., 3D -QSAR in drug design -a review. Current topics in medicinal  \\nchemistry 2010 , 10, 95 -115.  \\n37. Fan, F.; Warshaviak, D. T.; Hamadeh, H. K.; Dunn, R. T., The integration of pharmacophore -based 3D \\nQSAR modeling and virtual screening in safety profiling: A case study to identify antagonistic activities against \\nadenosine receptor, A2A, using 1,897 known drugs. PloS one 2019 , 14.  \\n38. Gladysz, R.; Dos Santos, F. M.; Langenaeker, W.; Thijs, G.; Augustyns, K.; De Winter, H., Spectrophores \\nas one -dimensional descriptors calculated from three -dimensional atomic properties: appli cations ranging \\nfrom scaffold hopping to multi -target virtual screening. Journal of cheminformatics 2018 , 10, 9.  \\n39. Nguyen, T. T.; Nguyen, N. D.; Nahavandi, S., Deep reinforcement learning for multiagent systems: A \\nreview of challenges, solutions, and app lications. IEEE Transactions on Cybernetics 2020 . \\n40. LeCun, Y.; Bengio, Y.; Hinton, G., Deep learning. nature 2015 , 521, 436 -444.  \\n41. Goodfellow, I.; Bengio, Y.; Courville, A., Deep learning . MIT press: 2016.  \\n42. Kleene, S. C. Representation of events in nerve nets and finite automata ; RAND PROJECT AIR FORCE \\nSANTA MONICA CA: 1951.  \\n43. LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P., Gradient -based learning applied to document recognition. \\nProceedings of the IEEE 1998 , 86, 2278 -2324.  \\n44. Rumelhart, D. E.; Hi nton, G. E.; Williams, R. J., Learning representations by back -propagating errors. \\nnature 1986 , 323, 533 -536.  \\n45. Hochreiter, S.; Schmidhuber, J., Long short -term memory. Neural computation 1997 , 9, 1735 -1780.  \\n46. Goodfellow, I.; Pouget -Abadie, J.; Mirza, M.; Xu, B.; Warde -Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. \\nGenerative adversarial nets. In Advances in neural information processing systems, 2014; 2014; pp 2672 -2680.  \\n47. Berman, H. M.; Westbrook, J.; Feng, Z.; Gilliland, G.; Bhat, T. N.; Weissig,  H.; Shindyalov, I. N.; Bourne, P. \\nE., The protein data bank. Nucleic acids research 2000 , 28, 235 -242.  \\n48. UniProt: the universal protein knowledgebase. Nucleic acids research 2017 , 45, D158 -D169.  \\n49. Kim, S.; Thiessen, P. A.; Bolton, E. E.; Chen, J.; Fu,  G.; Gindulyte, A.; Han, L.; He, J.; He, S.; Shoemaker, B. \\nA., PubChem substance and compound databases. Nucleic acids research 2016 , 44, D1202 -D1213.  \\n50. Gaulton, A.; Hersey, A.; Nowotka, M.; Bento, A. P.; Chambers, J.; Mendez, D.; Mutowo, P.; Atkinson, F .; \\nBellis, L. J.; Cibrián -Uhalte, E., The ChEMBL database in 2017. Nucleic acids research 2017 , 45, D945 -D954.  \\n51. Wishart, D. S.; Feunang, Y. D.; Guo, A. C.; Lo, E. J.; Marcu, A.; Grant, J. R.; Sajed, T.; Johnson, D.; Li, C.; \\nSayeeda, Z., DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic acids research 2018 , 46, \\nD1074 -D1082.  \\n52. Irwin, J. J.; Shoichet, B. K., ZINC− a free database of commercially available compounds for virtual \\nscreening. Journal of chemical information and modeling 2005, 45, 177 -182.  \\n53. Huang, Z.; Mou, L.; Shen, Q.; Lu, S.; Li, C.; Liu, X.; Wang, G.; Li, S.; Geng, L.; Liu, Y., ASD v2. 0: updated \\ncontent and novel features focusing on allosteric regulation. Nucleic acids research 2014 , 42, D510 -D516.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content=\"26 \\n 54. Nidhi; Glick,  M.; Davies, J. W.; Jenkins, J. L., Prediction of biological targets for compounds using \\nmultiple -category Bayesian models trained on chemogenomics databases. Journal of chemical information and \\nmodeling 2006 , 46, 1124 -1133.  \\n55. Wang, L.; Ma, C.; Wipf, P.;  Liu, H.; Su, W.; Xie, X. -Q., TargetHunter: an in silico target identification tool \\nfor predicting therapeutic potential of small organic molecules based on chemogenomic database. The AAPS \\njournal 2013 , 15, 395 -406.  \\n56. Weininger, D., SMILES, a chemical la nguage and information system. 1. Introduction to methodology \\nand encoding rules. Journal of chemical information and computer sciences 1988 , 28, 31 -36. \\n57. OEChem, T., OpenEye Scientific Software. Inc., Santa Fe, NM, USA 2012 . \\n58. Landrum, G., RDKit: Open -source cheminformatics. 2006 . \\n59. O’Boyle, N. M., Towards a Universal SMILES representation -A standard method to generate canonical \\nSMILES based on the InChI. Journal of cheminformatics 2012 , 4, 22.  \\n60. Durant, J. L.; Leland, B. A.; Henry, D. R.; Nourse, J. G., Reoptimization of MDL keys for use in drug \\ndiscovery. Journal of chemical information and computer sciences 2002 , 42, 1273 -1280.  \\n61. Rogers, D.; Hahn, M., Extended -connectivity fingerprints. Journal of chemical information and modeling \\n2010 , 50, 742 -754.  \\n62. Hert, J.; Willett, P.; Wilton, D. J.; Acklin, P.; Azzaoui, K.; Jacoby, E.; Schuffenhauer, A., Comparison of \\nfingerprint -based methods for virtual screening using multiple bioactive reference structures. Journal of \\nchemical information and compute r sciences 2004 , 44, 1177 -1185.  \\n63. Bian, Y.; Wang, J.; Jun, J. J.; Xie, X. -Q., Deep convolutional generative adversarial network (dcGAN) \\nmodels for screening and design of small molecules targeting cannabinoid receptors. Molecular pharmaceutics \\n2019 , 16, 4451 -4460.  \\n64. Elton, D. C.; Boukouvalas, Z.; Fuge, M. D.; Chung, P. W., Deep learning for molecular design —a review \\nof the state of the art. Molecular Systems Design & Engineering 2019 , 4, 828 -849.  \\n65. Wang, R.; Fang, X.; Lu, Y.; Yang, C. -Y.; Wang, S., Th e PDBbind database: methodologies and updates. \\nJournal of medicinal chemistry 2005 , 48, 4111 -4119.  \\n66. Papadatos, G.; Davies, M.; Dedman, N.; Chambers, J.; Gaulton, A.; Siddle, J.; Koks, R.; Irvine, S. A.; \\nPettersson, J.; Goncharoff, N., SureChEMBL: a larg e-scale, chemically annotated patent document database. \\nNucleic acids research 2016 , 44, D1220 -D1228.  \\n67. Gilson, M. K.; Liu, T.; Baitaluk, M.; Nicola, G.; Hwang, L.; Chong, J., BindingDB in 2015: a public database \\nfor medicinal chemistry, computational ch emistry and systems pharmacology. Nucleic acids research 2016 , 44, \\nD1045 -D1053.  \\n68. Ruddigkeit, L.; Van Deursen, R.; Blum, L. C.; Reymond, J. -L., Enumeration of 166 billion organic small \\nmolecules in the chemical universe database GDB -17. Journal of chemic al information and modeling 2012 , 52, \\n2864 -2875.  \\n69. Heller, S. R.; McNaught, A.; Pletnev, I.; Stein, S.; Tchekhovskoi, D., InChI, the IUPAC international \\nchemical identifier. Journal of cheminformatics 2015 , 7, 23.  \\n70. Glen, R. C.; Bender, A.; Arnby, C. H .; Carlsson, L.; Boyer, S.; Smith, J., Circular fingerprints: flexible \\nmolecular descriptors with applications from physical chemistry to ADME. IDrugs 2006 , 9, 199.  \\n71. Pérez -Nueno, V. I.; Rabal, O.; Borrell, J. I.; Teixidó, J., APIF: a new interaction fin gerprint based on atom \\npairs and its application to virtual screening. Journal of chemical information and modeling 2009 , 49, 1245 -\\n1260.  \\n72. O'Boyle, N. M.; Banck, M.; James, C. A.; Morley, C.; Vandermeersch, T.; Hutchison, G. R., Open Babel: \\nAn open chemi cal toolbox. Journal of cheminformatics 2011 , 3, 33.  \\n73. Willighagen, E. L.; Mayfield, J. W.; Alvarsson, J.; Berg, A.; Carlsson, L.; Jeliazkova, N.; Kuhn, S.; Pluskal, T.; \\nRojas -Chertó, M.; Spjuth, O., The Chemistry Development Kit (CDK) v2. 0: atom typing , depiction, molecular \\nformulas, and substructure searching. Journal of cheminformatics 2017 , 9, 33.  \"),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='27 \\n 74. Ambure, P.; Aher, R. B.; Roy, K. Recent advances in the open access cheminformatics toolkits, software \\ntools, workflow environments, and databases. In Computer -Aided Drug Discovery ; Springer: 2014, pp 257 -296.  \\n75. Arabie, P.; Baier, N. D.; Critchley, C. F.; Keynes, M., Studies in Classification, Data Analysis, and \\nKnowledge Organization. 2006 . \\n76. Warr, W. A., Scientific workflow systems: Pipeline Pilot and KNIME. Journal of computer -aided \\nmolecular design 2012 , 26, 801 -804.  \\n77. Beisken, S.; Meinl, T.; Wiswedel, B.; de Figueiredo, L. F.; Berthold, M.; Steinbeck, C., KNIME -CDK: \\nWorkflow -driven cheminformatics. BMC bioinformatics 2013 , 14, 257.  \\n78. Saubern,  S.; Guha, R.; Baell, J. B., KNIME workflow to assess PAINS filters in SMARTS format. \\nComparison of RDKit and Indigo cheminformatics libraries. Molecular informatics 2011 , 30, 847 -850.  \\n79. Roughley, S. D., Five Years of the KNIME Vernalis Cheminformatics C ommunity Contribution. Current \\nmedicinal chemistry 2019 . \\n80. Abadi, M.; Barham, P.; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, \\nM. Tensorflow: A system for large -scale machine learning. In 12th {USENIX} Symposium on  Operating Systems \\nDesign and Implementation ({OSDI} 16), 2016; 2016; pp 265 -283.  \\n81. Etaati, L. Deep Learning Tools with Cognitive Toolkit (CNTK). In Machine Learning with Microsoft \\nTechnologies ; Springer: 2019, pp 287 -302.  \\n82. Team, T.; Al -Rfou, R.; Alai n, G.; Almahairi, A.; Angermueller, C.; Bahdanau, D.; Ballas, N.; Bastien, F.; \\nBayer, J.; Belikov, A.; Belopolsky, A.; Bengio, Y.; Bergeron, A.; Bergstra, J.; Bisson, V.; Bleecher Snyder, J.; \\nBouchard, N.; Boulanger -Lewandowski, N.; Bouthillier, X.; Zhang,  Y., Theano: A Python framework for fast \\ncomputation of mathematical expressions. 2016 . \\n83. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; \\nAntiga, L. PyTorch: An imperative style, high -performan ce deep learning library. In Advances in Neural \\nInformation Processing Systems, 2019; 2019; pp 8024 -8035.  \\n84. Chollet, F., In; 2015.  \\n85. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, \\nP.; Weiss, R.; Dubourg, V., Scikit -learn: Machine learning in Python. the Journal of machine Learning research \\n2011 , 12, 2825 -2830.  \\n86. Mikolov, T.; Karafiát, M.; Burget, L.; Černocký, J.; Khudanpur, S. Recurrent neural network based \\nlanguage model. In Eleventh annual conference of the international speech communication association, 2010; \\n2010.  \\n87. Mikolov, T.; Kombrink, S.; Burget, L. ; Černocký, J.; Khudanpur, S. Extensions of recurrent neural \\nnetwork language model. In 2011 IEEE international conference on acoustics, speech and signal processing \\n(ICASSP), 2011; IEEE: 2011; pp 5528 -5531.  \\n88. Mikolov, T.; Zweig, G. Context dependent rec urrent neural network language model. In 2012 IEEE \\nSpoken Language Technology Workshop (SLT), 2012; IEEE: 2012; pp 234 -239.  \\n89. Hanson, J.; Yang, Y.; Paliwal, K.; Zhou, Y., Improving protein disorder prediction by deep bidirectional \\nlong short -term memory recurrent neural networks. Bioinformatics 2017 , 33, 685 -692.  \\n90. Cheng, J.; Dong, L.; Lapata, M., Long short -term memory -networks for machine reading. arXiv preprint \\narXiv:1601.06733 2016 . \\n91. Gupta, A.; Müller, A. T.; Huisman, B. J.; Fuchs, J. A.; Schneid er, P.; Schneider, G., Generative recurrent \\nnetworks for de novo drug design. Molecular informatics 2018 , 37, 1700111.  \\n92. Bian, Y.; Xie, X. -Q. S., Computational fragment -based drug design: Current trends, strategies, and \\napplications. The AAPS journal 2018, 20, 59.  \\n93. Segler, M. H.; Kogej, T.; Tyrchan, C.; Waller, M. P., Generating focused molecule libraries for drug \\ndiscovery with recurrent neural networks. ACS central science 2018 , 4, 120 -131.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='28 \\n 94. Moret, M.; Friedrich, L.; Grisoni, F.; Merk, D.; Schneid er, G., Generative molecular design in low data \\nregimes. Nature Machine Intelligence 2020 , 2, 171 -180.  \\n95. Merk, D.; Friedrich, L.; Grisoni, F.; Schneider, G., De novo design of bioactive small molecules by \\nartificial intelligence. Molecular informatics 2018, 37, 1700153.  \\n96. Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y., Empirical evaluation of gated recurrent neural networks on \\nsequence modeling. arXiv preprint arXiv:1412.3555 2014 . \\n97. Zheng, S.; Yan, X.; Gu, Q.; Yang, Y.; Du, Y.; Lu, Y.; Xu, J., QBMG: qu asi-biogenic molecule generator with \\ndeep recurrent neural network. Journal of cheminformatics 2019 , 11, 5.  \\n98. Kramer, M. A., Nonlinear principal component analysis using autoassociative neural networks. AIChE \\njournal 1991 , 37, 233 -243.  \\n99. Kingma, D. P.;  Welling, M., Auto -encoding variational bayes. arXiv preprint arXiv:1312.6114 2013 . \\n100.  Kingma, D. P.; Welling, M., An introduction to variational autoencoders. arXiv preprint \\narXiv:1906.02691 2019 . \\n101.  Kingma, D. P.; Mohamed, S.; Rezende, D. J.; Welling , M. Semi -supervised learning with deep generative \\nmodels. In Advances in neural information processing systems, 2014; 2014; pp 3581 -3589.  \\n102.  Khemakhem, I.; Kingma, D. P.; Hyvärinen, A., Variational autoencoders and nonlinear ica: A unifying \\nframework. arXiv preprint arXiv:1907.04809 2019 . \\n103.  Pu, Y.; Gan, Z.; Henao, R.; Yuan, X.; Li, C.; Stevens, A.; Carin, L. Variational autoencoder for deep \\nlearning of images, labels and captions. In Advances in neural information processing systems, 2016; 2016; pp \\n2352-2360.  \\n104.  Gómez -Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernández -Lobato, J. M.; Sánchez -Lengeling, B.; \\nSheberla, D.; Aguilera -Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru -Guzik, A., Automatic chemical design \\nusing a data -driven continuous r epresentation of molecules. ACS central science 2018 , 4, 268 -276.  \\n105.  Blaschke, T.; Olivecrona, M.; Engkvist, O.; Bajorath, J.; Chen, H., Application of generative autoencoder \\nin de novo molecular design. Molecular informatics 2018 , 37, 1700123.  \\n106.  Sattarov, B.; Baskin, I. I.; Horvath, D.; Marcou, G.; Bjerrum, E. J.; Varnek, A., De novo molecular design \\nby combining deep autoencoder recurrent neural networks with generative topographic mapping. Journal of \\nchemical information and modeling 2019 , 59, 1182 -1196.  \\n107.  Samanta, B.; Abir, D.; Jana, G.; Chattaraj, P. K.; Ganguly, N.; Rodriguez, M. G. Nevae: A deep generative \\nmodel for molecular graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019; 2019; Vol. \\n33; pp 1110 -1117.  \\n108.  Simon ovsky, M.; Komodakis, N. Graphvae: Towards generation of small graphs using variational \\nautoencoders. In International Conference on Artificial Neural Networks, 2018; Springer: 2018; pp 412 -422.  \\n109.  Imrie, F.; Bradley, A. R.; van der Schaar, M.; Deane, C.  M., Deep Generative Models for 3D Linker Design. \\nJournal of Chemical Information and Modeling 2020 , 60, 1983 -1995.  \\n110.  Makhzani, A.; Shlens, J.; Jaitly, N.; Goodfellow, I.; Frey, B., Adversarial autoencoders. arXiv preprint \\narXiv:1511.05644 2015 . \\n111.  Kadurin, A.; Nikolenko, S.; Khrabrov, K.; Aliper, A.; Zhavoronkov, A., druGAN: an advanced generative \\nadversarial autoencoder model for de novo generation of new molecules with desired molecular properties in \\nsilico. Molecular pharmaceutics 2017 , 14, 3098 -3104. \\n112.  Polykovskiy, D.; Zhebrak, A.; Vetrov, D.; Ivanenkov, Y.; Aladinskiy, V.; Mamoshina, P.; Bozdaganyan, M.; \\nAliper, A.; Zhavoronkov, A.; Kadurin, A., Entangled conditional adversarial autoencoder for de novo drug \\ndiscovery. Molecular pharmaceutics 2018, 15, 4398 -4405.  \\n113.  Shayakhmetov, R.; Kuznetsov, M.; Zhebrak, A.; Kadurin, A.; Nikolenko, S.; Aliper, A.; Polykovskiy, D., \\nMolecular Generation for Desired Transcriptome Changes With Adversarial Autoencoders. Frontiers in \\nPharmacology 2020 , 11, 269.  '),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='29 \\n 114. Guimaraes, G. L.; Sanchez -Lengeling, B.; Outeiral, C.; Farias, P. L. C.; Aspuru -Guzik, A., Objective -\\nreinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint \\narXiv:1705.10843 2017 . \\n115.  Maziarka, Ł.; Pocha, A.; Kaczmarczyk, J.; Rataj, K.; Danel, T.; Warchoł, M., Mol -CycleGAN: a generative \\nmodel for molecular optimization. Journal of Cheminformatics 2020 , 12, 1 -18. \\n116.  Méndez -Lucio, O.; Baillif, B.; Clevert, D. -A.; Rouquié, D.; Wicha rd, J., De novo generation of hit -like \\nmolecules from gene expression signatures using artificial intelligence. Nature Communications 2020 , 11, 1 -10. \\n117.  Prykhodko, O.; Johansson, S. V.; Kotsias, P. -C.; Arús -Pous, J.; Bjerrum, E. J.; Engkvist, O.; Chen, H ., A de \\nnovo molecular generation method using latent vector based generative adversarial network. Journal of \\nCheminformatics 2019 , 11, 74.  \\n118.  Huang, G.; Liu, Z.; Van Der Maaten, L.; Weinberger, K. Q. Densely connected convolutional networks. In \\nProceedi ngs of the IEEE conference on computer vision and pattern recognition, 2017; 2017; pp 4700 -4708.  \\n119.  LeCun, Y.; Bengio, Y., Convolutional networks for images, speech, and time series. The handbook of \\nbrain theory and neural networks 1995 , 3361, 1995.  \\n120.  Yu, D.; Wang, H.; Chen, P.; Wei, Z. Mixed pooling for convolutional neural networks. In International \\nconference on rough sets and knowledge technology, 2014; Springer: 2014; pp 364 -375.  \\n121.  Radford, A.; Metz, L.; Chintala, S., Unsupervised representatio n learning with deep convolutional \\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434 2015 . \\n122.  Zhang, H.; Goodfellow, I.; Metaxas, D.; Odena, A., Self -attention generative adversarial networks. arXiv \\npreprint arXiv:1805.08318 2018 . \\n123.  Li, C.; Wand, M. Precomputed real -time texture synthesis with markovian generative adversarial \\nnetworks. In European conference on computer vision, 2016; Springer: 2016; pp 702 -716.  \\n124.  Gao, W.; Coley, C. W., The synthesizability of molecules proposed by gen erative models. Journal of \\nChemical Information and Modeling 2020 . \\n125.  Coley, C. W.; Rogers, L.; Green, W. H.; Jensen, K. F., SCScore: synthetic complexity learned from a \\nreaction corpus. Journal of chemical information and modeling 2018 , 58, 252 -261.  \\n126. Vargesson, N., Thalidomide ‐induced teratogenesis: History and mechanisms. Birth Defects Research \\nPart C: Embryo Today: Reviews 2015 , 105, 140 -156.  \\n127.  Polishchuk, P. G.; Madzhidov, T. I.; Varnek, A., Estimation of the size of drug -like chemical space ba sed \\non GDB -17 data. Journal of computer -aided molecular design 2013 , 27, 675 -679.  \\n128.  Alley, E. C.; Khimulya, G.; Biswas, S.; AlQuraishi, M.; Church, G. M., Unified rational protein engineering \\nwith sequence -based deep representation learning. Nature meth ods 2019 , 16, 1315 -1322.  \\n ')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('gan_drug_discovery.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 0}, page_content='1 \\n Generative chemistry: drug discovery with deep learning generative models  \\n \\nYuemin Bian1,2 and Xiang -Qun Xie1,2,3,4* \\n \\n1Department of Pharmaceutical Sciences and Computational Chemical Genomics Screening Center, School of \\nPharmacy; 2NIH National Center of Exc ellence for Computational Drug Abuse Research; 3Drug Discovery \\nInstitute; 4Departments of Computational Biology and Structural Biology, School of Medicine, University of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 0}, page_content='Pittsburgh, Pittsburgh, Pennsylvania 15261, United States.  \\n \\n \\n*Corresponding Author: Xi ang-Qun Xie, MBA, Ph.D.  \\nProfessor of Pharmaceutical Sciences/Drug Discovery Institute  \\nDirector of CCGS and NIDA CDAR Centers  \\n335 Sutherland Drive, 206 Salk Pavilion  \\nUniversity of Pittsburgh  \\nPittsburgh, PA15261, USA  \\n412-383-5276 (Phone)  \\n412-383-7436 (Fax)  \\nEmail: xix15@pitt.edu'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 1}, page_content='2 \\n Abstract :  \\nThe de novo  design of molecular structures using deep learning generative models introduces an encouraging \\nsolution to drug discovery in the face of the continuously increased cost of new drug development. From the \\ngenera tion of original texts, images, and videos, to the scratching of novel molecular structures, the incredible  \\ncreativity of deep learning generative models surprise d us about the height machine intelligence can achieve.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 1}, page_content='The purpose of this paper is to review  the latest advances in generative chemistry which relies on generative \\nmodeling to expedite the drug discovery process.  This review starts with a brief history of artificial intelligence  \\nin drug discovery to outline this emerging paradigm. Commonly used c hemical databases, molecular \\nrepresentations, and tools in  cheminformatics and machine learning are covered as the infrastructure  for the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 1}, page_content='generative chemistry . The detailed discussion s on utilizing  cutting -edge generative architectures,  including  \\nrecurrent  neural network , variational autoencoder , adversarial autoencoder , and generative adversarial network  \\nfor compound generation are focused . Challenges and f uture perspectives  follow .  \\n \\nKeywords : Drug discovery, Deep learning, Generative model, Recurrent neu ral network, Variation al \\nautoencoder, Adversarial autoencoder, Generative adversarial network'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='3 \\n 1. INTRODUCTION  \\nDrug discovery is expensive. The average cost for the development of a new drug now hits 2.6 billion USD and \\nthe overall discovery process takes over 12 years to finish1, 2. Moreover, these numbers keep  increasing. It is \\ncritical to think and explore efficient and effective strategies t o confront the growing cost and to accelerate the \\ndiscovery process. The progression in the high -throughput screening (HTS) dramatically speeded up the task of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='lead identification by screening candidate compounds in large volume3, 4. When it comes to the lead \\nidentification, the concept can be further classified into two divisions, the structure -based approach5, 6 and the \\nligand -based approach7. Combining  with the significant progress  in computation, the development of thes e two \\napproaches has resulted in constructive virtual screening (VS) methodologies. Traditionally, with the structure'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='of the target protein available, structure -based approaches including molecular docking studies8-10, molecular \\ndynamic simulations11-13, fragment -based approach10, 14, etc. can be applied to explore the potential receptor -\\nligand interactions and to virtually screen a large compound set for finding the plausible lead. Then with the \\nidentified active molecules for the given target, ligand -based approaches such as pharmac ophore modeling15, 16,'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='scaffolding hopping17, 18, and molecular fingerprint similarity search19 can be conducted for modifying known \\nleads and f or finding future compounds. The rapid advancement in computational power and the blossom of \\nmachine learning (ML) algorithms brought the ML -based decision -making model20, 21 as an alternative path to \\nthe VS campaigns in the past decades. There is increased availability of data in cheminformatics and drug'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='discovery. The capability of dealing with large data  to dete ct hidden patterns and to facilitate the future data \\nprediction in a time -efficient manner  favored ML in building VS pipelines.  \\n It is encouraging to note t he successful applications of the above -mentioned computational chemistry \\napproaches and ML -based V S pipelines on drug discovery these days. The conventional methods are effective.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='However, the challenge  remains  on developing pioneering methods, techniques, and strategies in the \\nconfrontation of the costly procedure of drug discovery. The flourishing of  deep le arning generative models \\nbrings fresh  solution s and opportunities to this field. From the generated human faces that are indistinguishable \\nwith real people22, to the text generation tools that mimic the tone and vocabulary of certain authors23, the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='astonishing creativity of deep learning generative models brings our understanding of the machine intelligence \\nto a new level. In recen t years, the expeditions toward  genera tive chemistry mushroomed, which explore d the \\npossibility of utilizing generative models to effectively and efficiently design molecular structures with desired \\nproperties. Promising and compelling outcomes including the identification of DDR1 kinase inhib itors within'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 2}, page_content='21 days using deep learning generative models24 may indicate that we are probably at  the corner of an upcoming \\nrevolution of drug discovery in the artificial intelligence (AI) era. This review article starts with a brief \\nevolution of AI in drug discovery , and the infrastructures in both cheminformatics and machine learning. The \\nstate-of-the-art generative models including recurrent neural networks (RNNs), variational autoencoders'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='4 \\n (VAEs), adversarial autoencoders (AAEs), and generative adversarial networks (GANs) are focused on to \\ndiscuss their fundamental architectures as well as their app lications in  the de novo  drug design.  \\n \\n2. ARTIFICIAL INTELLIGENCE  IN DRUG DISCOVERY  \\nArtificial intelligence (AI) is the study of developing and implementing techniques that enable  the machine to \\nbehave with human -like intelligence25. The conc ept of AI can be traced back to  the 1950s  when researches'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='questioned  whether c omputers can be made to handle automate d intelligence tasks which are commonly \\nfulfilled by humans26. Thus, AI is a broad area of research that includes both (1) methodologies employing  \\nlearning processes  and (2) approaches that no learning process is involved  in. At the early stage, researchers \\nbelie ved that by defining  a sufficient number  of explicit rules to maneuver knowledge, the human -level AI can'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='be expected  (Fig. 1a ). In the face of a specific problem, the human studying process on existing observations \\ncan contribute to the accumulation of knowledge. Explicit rules were expected to describe knowledge. By \\nprogramming and applying these rules, the answers for future observations are anticipated. This strategy is also \\nknown as symbolic AI27. Symbolic AI is an efficient solution to logical problems, for instance, chess playing.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='However, when handling problems with blurry, unclear, and distorted knowledge, such as image recognition, \\nlanguage tr anslation, and to our topic , the classification of active compounds from decoys for a therapeutic \\ntarget, symbolic AI turne d out to show limited capa bility. We may define explicit rules to guide the selection of \\ngeneral drug -like compounds, Lipinski’s rule  of five28 for example, but it is almost impossible to exhaust'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='specified rules for guiding  the s election of agonists to cannabinoid receptor 2 or other targets29. Machine \\nlearning (ML) took over symbolic AI’s position as a novel method with the ability to learn on its own.  \\n ML allows computers to solve specific tasks by  learn ing on their own30, 31. Through  directly looking at \\nthe data , computers can summarize  the rules instead of waiting  for programmers to craft them ( Fig. 1b ). In the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='paradigm of ML -based problem solving , data and the answers to the data are functioned as input with rules  as \\nthe outcome. The produced rules can then be applied to predict answers for future data. Statistical analysis is \\nalways associated with ML, while they can be distinguished at several aspects32. The application of ML is \\nusually towards large and complex datasets, such as a datase t with millions of small molecules that cover  a huge'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='chemical space with diversified scaffolds, which statistical analysis can be incapable to deal with. The flourish \\nof ML starts in the 1990s33. The method rapidly became a dominant player in the field of AI. Commonly used  \\nML systems  in drug discovery  can be categorized into supervised learning, unsupervised learning, and \\nreinforcement learning  (Fig. 1 c). In supervised learning, the algorithms are fed with both the data and the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 3}, page_content='answers to these data (label). Protein family subtypes selectivity prediction is an example for classification: the \\nclassifier is trained with numbers of sample molecules  along with their labels (the specific protein family \\nmember they interact with), and the well -trained classifier should be able to classify the future molecules20, 29, 34, \\n35. Quantitative structure -activity relationship analysis is an example for regression: the regress or is trained with'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 4}, page_content='5 \\n molecules sharing similar scaffold along with their biological activity data ( Ki, IC50, and EC 50 values for \\nexample), and the well -trained regressor should be able to predict the numeric activity value s for future \\nmolecules with the similar scaffold10, 36. In unsupervised learning, the algorithms  are trained with unlabeled data . \\nFor instance, a high -throughput screening campaign may preselect a smaller representative compound set from'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 4}, page_content='a large compound database using the clustering method to group molecu les with similar structures into \\nclusters37, 38. A subset of molecules selected from different clusters can then offer improved  structural diversity \\nto cover a bigger chemical space than random pickup. In reinforcement  learning , the learning system can choose \\nactions acco rding to its observation of the environment, and get a penalty (or reward) in return39. To achieve the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 4}, page_content='lowest penal ty (or highest reward), the system must learn and choose the best strategy by itself.  \\n \\n \\nFigure 1. From artificial i ntelligence to deep learning. a . The programmin g paradigm for symbolic AI. b. T he \\nprogramming paradigm for ML. c . The relationship among  artificial intelligence, machine learning, and deep \\nlearning.  \\n \\n Deep learning (DL) is a speci fic subfield of ML that adapts  neural networks to emphasize the learning'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 4}, page_content='processes with successive layers  (Fig. 1 c). DL methods can transfer the representation at on e level to a higher \\nand more abstract level40. The feature of representation learning enables DL methods to discover \\nrepresentations from the raw input data for tasks such as detection and classification.  The word “deep” in DL \\nreflects this character of successive layers of representations, and  the number of layers determines  the depth of a'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='6 \\n DL model41. In contrast, conventional ML methods that transform the input data into one or two successive \\nrepresentation spaces are sometimes referred to as shallow learning methods. The vas t development in the past \\ndecades brought DL great flexibility on the selection of architectures , such as the fully connected artificial \\nneural network (ANN) or multi -layer perceptron (MLP)42, convolutional neural network (CNN)43, and recurrent'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='neural network (RNN)44. The rise of g enerative chemistry is largely benefited from the  extensive advancement \\nof generative modeling, which predominantly depends on the flourishing of DL architectures. The successful \\napplication of  the Long Short -Term Memory (LSTM) model45, a special type of RNN model, on text generation \\ninspired the simplified molecular -input line -entry system (SMILES) -based compound  design. And the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='promi sing exercise of using  the Generative Adversarial Network (GAN) model46 for image generation \\nmotivated the fingerprint  and graph  centered molecular structural scratch. The major reason for D L to bloom \\nrapidly can be that the very method provides solution s to previously unsolvable problem s and outperforms the \\ncompetitors with simplified representation learning  process26, 40. It is foreseen that the process of molecule'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='design can evolve into a more efficient and effective manner with the proper fusion with DL.  \\n \\n3. DATA SOURCES  AND  MACHINE LEARNING INFRAST RUCTURES  \\nDeep learning campaigns start with high-quality input data . The successful development of generative \\nchemistry models relies on cheminformatics and bioinformatics data for the molecules and biological systems. \\nTable 1  exhib its some routinely  used databases in drug discovery for both small and large biological molecules.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='In a typical case of structure -based drug discovery, a 3D model of the protein (or DNA/RNA) target is critical \\nfor the following steps  on evaluating potentia l receptor -ligand interactions. PDB database47 is a good source for \\naccessing structural information for large biological systems, and the UniProt database48 will be a convenient \\nsource for sequence data. Regarding chemicals, PubChem49 can be a go-to place. PubChem is comprehensive. I t'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='currently contains ~103 million compounds (with unique chemical structures ) and ~253 million substances \\n(information about chemical entities). If the major focus is on bioactive molecules, ChEMBL50 can be an \\nefficient database to interact. ChEMBL currently documents ~2 million reported drug -like compounds with \\nbioactivity data  for 13,000 targets . Supposing that the interes t is more on studying the existing drugs on the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='market instead of drug -like compounds, the DrugBank51 can serve. To date, DrugBank records ~ 14,000 drugs, \\nincluding ap proved small molecule drugs and biologics, nutraceuticals , and discovery -phase drugs. With virtual \\nscreening campaigns, adding some commercially available compounds to in -house libraries are preferred as \\nthey may further increase the structural diversity a nd expand the coverage of the chemical space. Once potential'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 5}, page_content='hits were predicted to be among these compounds, the commercial availability gives them easy access for future \\nexperimental validations. Zinc database52 now archives ~230 million purchasable compounds in ready -to-dock \\nformat. It is worth mentioning that constructing topic -specific and tar get-specific databases is trending. ASD53 \\nis one example that files allosteric modulators and related macromolecu les to facilitate the research  on allosteric'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 6}, page_content='7 \\n modulation. The rising of Chemogenomics databases54, 55 for certain diseases and dru ggable targets is another \\nexample that these libraries focus on particular  areas of research .  \\n \\nWith the input data ready, the next consideration is transforming the data into machine -readable format. \\nTable 2  lists commonly used molecular representations. SMILES56 describes molecular structures in a text -'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 6}, page_content='based format using short ASCI I strings. Multiple SMILES strings can be generated for the same molecule with \\ndifferent starting atoms. This ambiguity  led to the effects of canonicalization  that determines which of all \\npossible SMILES will be used as the reference SMILES for a molecule.  Popular cheminformatics packages \\nsuch as OpenEye57 and RDKit58 are possible solutions for standardizing the canonical SMILES59. The canonical'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 6}, page_content='SMILES is a type of well -liked molecular representation in generative chemistry models as it packs well with \\nlanguage processing and sequence generation techniques like RNNs. Usually the SMILES strings are fi rst \\nconverted with one -hot encoding. The categorical distribution for each element can then be produced by the \\ngenerative models. Fingerprints are another vital group of molecular representations. Molecular Access System'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 6}, page_content='(MACCS) fingerprint60 has 166 binary keys, each of which indicates the presence of one of the 166 MDL \\nMACCS structural keys calculated from the molecular graph.  Fingerprints can be calculated th rough different \\napproaches. By  enumerating circular fragments, linear fragments, and tree fragments from the molecular graph, \\nCircular61, Path, and Tree fingerprints62 can be created.  Using fingerprints as representations may suffer from'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 6}, page_content='inconvertibility in that the complete structure of a molecule cannot be reconstructed directly from the \\nfingerprints63. To have fingerprints calculated for a large enough compounds library to fu nction as a look -up \\nindex may be a compromised solution64. Despite thi s difficulty, fingerprints are popular among ML \\nclassification models for tasks like distinguishing active compounds from inactive ones for a given target.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='8 \\n Table 1. Well established cheminformatics databases available for drug discovery  \\nDatabase  Descrip tion Web linkage  Examples of usage  \\nUniProt48 The Universal Protein Resource \\n(UniProt) is a resource for protein \\nsequence and annotation data  https://www.unip\\nrot.org  Protein sequence homology search, \\nalignment, and protein ID retrieving \\nespecially for structural -based drug \\ndiscov ery \\nRCSB PDB47 The Protein Data Bank (PDB) provides \\naccess to 3D structure data for large'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='access to 3D structure data for large \\nbiological molecules, including \\nprotein, DNA, and RNA.  https://www.rcsb\\n.org Protein 3D structures are fundamental for \\nhot spot identification, docking simulation, \\nand molecular dynamics simulation in \\nstructural -based drug discovery.  \\nPDBbind65 PDBbind provides a collection of the \\nexperimentally measured binding \\naffinity data for all types of \\nbiomolecular complexes deposited in \\nthe PDB.  http://www.pdbb'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='the PDB.  http://www.pdbb\\nind.org.cn  The receptor -ligand binding data for \\nresolved protein structures can functi on as \\nthe benchmark to evaluate future \\nsimulations  \\nPubChem49 PubChem is the world’s largest \\ncollection of chemical information.  https://pubchem.\\nncbi.nlm.nih.gov  To acquire comprehensive chemical \\ninfor mation ranging from NMR spectra, \\nphysical -chemical properties, to \\nbiomolecular interactions.  \\nChEMBL50 ChEMBL is a manually curated'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='ChEMBL50 ChEMBL is a manually curated \\ndatabase of bioactive molecules with \\ndrug-like p roperties.  https://www.ebi.\\nac.uk/chembl/  To collect cheminformatics data of \\nreported molecules for a given target. A \\nhigh-quality compound collection is the \\nkey to the ligand -based drug discovery  \\nSureChEMBL66 SureChEMBL is a resource containing \\ncompounds e xtracted from patent \\nliterature . https://www.sure\\nchembl.org/searc\\nh/ Compound -patent associations'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='h/ Compound -patent associations  \\nBindingDB67 BindingDB  is a database  of measured \\nbinding affinities  for the interactions of \\nprotein considered to be drug -targets \\nwith small, drug -like molecules.  https://www.bind\\ningdb.org/bind/in\\ndex.jsp  To retrieve compound sets for a specific \\ntarget similar to ChEMBL but with  the \\nfocus on experimental binding affinities.  \\nDrugBank51 The DrugBank database combines \\ndetailed drug data with comprehensive'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='detailed drug data with comprehensive \\ndrug target information  https://www.dru g\\nbank.ca  Drug repurposing study for existing drugs. \\nOn-target and off -target analysis for a \\ncompound.  \\nZINC52 Zinc is a database of commercially -\\navailable compounds  https://zinc.docki\\nng.org  Zinc database is good for virtual screening \\non hit identification as the compounds are \\ncommercially available for quick \\nbiological vali dations afterward s. \\nEnam ine Enamine provides an enumerated'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='Enam ine Enamine provides an enumerated \\ndatabase of synthetically feasible \\nmolecules  https://enamine.n\\net The establishment of a target -specific \\ncompound library. Fragment -based drug \\ndiscovery.  \\nASD53 Allosteric Database (ASD)  provides a \\nresource for structure, function, disease \\nand related annotation for allosteric \\nmacromolecules and allosteric \\nmodulators  http://mdl.shsmu.\\nedu.cn/ASD/  To facilitate the research on allosteric \\nmodulation with enriched chemical data on'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 7}, page_content='modulation with enriched chemical data on \\nallosteric modulators.  \\nGDB68 GDB databases provide  multiple \\nsubsets of combinatorially generated \\ncompounds following chemical \\nstability and synthetic feasibility rules  http://gdb.unibe.\\nch/downloads/  Using combinator ial chemistry is a good \\nway to largely expand the chemical space.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='9 \\n Table 2. Examples of c ommonly used molecular representations  \\nRepresentation  Description  \\nSMILES56 The simplified molecular -input line -entry system  (SMILES) is a specification in the \\nform of a  line notation  for describing the structure of  chemical species  using \\nshort ASCII  strings.  \\nCanonical SMILES  Canonicalization is a way to determine which of all possible SMILES will be used \\nas the reference SMILES for a molecular graph.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='as the reference SMILES for a molecular graph.  \\nInChI69 The International Chemical Identifier  (InChI) is a textual  identifier for  chemical \\nsubstances, designed to provide a standard way to encode molecu lar information.  \\nInChI Key  The condensed, 27 character  InChI Key  is a hashed  version of the full InChI.  \\nFingerprints  MACCS \\nKeys60 MACCS keys are 166 bit  structural key  descripto rs in which each bit is associated \\nwith a S MART S pattern.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='with a S MART S pattern.  \\nCircular61, 70  Circular fingerprints are created by  exhaustively  enumerating  all circular fragments \\ngrown radially from each heavy atom of the molecule up to the given radius.  \\nPath62  Path fingerprints are created by  exhaustively  enumerating  all linear fragments of a \\nmolecular graph up to a given size . \\nTree62  Tree fingerprints are generated by  exhaustively  enumerating  all tree fragments of a \\nmolecular graph up to a given size .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='molecular graph up to a given size . \\nAtom Pair71 Atom  Pair fingerprints encode each atom as a type, enumerates all  \\ndistances between pairs, and then hashes the results.  \\n \\nAfter collecting the high -quality data and transforming the data into the appropriate format, it is time to \\napply data science to the development of the predictive  models. Table 3  illustrates examples of frequently \\nconsidered cheminformatics toolkits and machine learning packages. RDKit, Open Babel72, and CDK73 are'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='cheminformatics toolkits that  are comprised of a set of libraries with source codes for various functions , such as \\nchemical files I/O formatting, substructure and pattern search, and molecular representations g eneration . The \\ntypical appli cations of deploying these toolkits can contribute to virtual screening, structural similarity search, \\nstructure -activity relationship analysis, etc74. The w orkflo w environment is not unique t o the cheminformatics'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='research , but can facilitate the automation of data processing with a user-friendly interface. The workflow \\nsystems like KNIME75, 76 can execute tasks in succession and perform recurring tasks efficiently, such as \\niterative fingerprints calc ulation for a compound library.  The strategy of integrating cheminformatics toolkits as \\nnodes into a workflow and connecting them with edges is gaining  popularity and is increasingly employed77-79.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='When it comes to ML and DL modeling, TensorFlow80, CNTK81, Theano82, and PyTorch83 are well -recognized \\npackages for employment. These packages handle low -level operations inc luding tensor manipulation and \\ndifferentiation. In contrast , Keras84 is a model -level library that deals with tasks in a modular w ay. As a high -\\nlevel API, Keras is running on top of TensorFlow, CNTK, and Theano. Scikit -Learn85 is an efficient and'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 8}, page_content='straightforward tool for predictive data analysis. It is known more for its role in conventio nal ML modeling as'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='10 \\n the library comprehensively integrates algorithms like Support Vector Machine ( SVM ), Random Forest  (RF) , \\nLogistic regression, Naïve Bayes  (NB) , etc.  \\n \\nTable 3 . Commonly used cheminformatics and machine learning packages  \\nPackage  Descripti on Web linkage  \\nRDKit58 RDKit is an open -source toolkit for ch eminformatics. Features \\ninclude  2D and 3D molecular operations, descriptor generation, \\nmolecular database  cartridge, etc.  https://www.rdkit.org'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='Open Babel72 Open Babel is an open chemical toolbox to search, convert, \\nanalyze, or store data from molecular modeling, chemistry, solid -\\nstate materials, biochemistry, or related areas.  http://openbabel.org/wik\\ni/Main_Page  \\nCDK73 The Chemistry Development Kit (CDK)  is a collection of modular \\nJava libraries for processing cheminformatic s. https://cdk.github.io  \\nKNIME75 KNIM E is a workflow environment in data science that can be'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='integrated to automate certain cheminformatics operations.  https://www.knime.com  \\nTensorFlow80 TensorFlow is an open -source platform for machine learning. It \\nhas a  set of tools, libraries , and community resources that enable \\nresearchers to build and deploy ML applications.  https://www.tensorflow.\\norg \\nCNTK81 The Cognitive Toolkit (CNTK) is an open -source toolkit for \\ncommercial -grade distributed deep learning. It describes neural'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='networks as a  series of computational steps via a directed graph.  https://github.com/micro\\nsoft/CNTK  \\nTheano82 Theano is a Python library for defining, optimizing, and \\nevaluating mathematical expressions.  http://deeplearning.net/s\\noftware/theano/  \\nPyTorch83 PyTorch is an open -source machine learning library based on the \\nTorch library.  https://pytorch.org  \\nKeras84 Keras is a high -level neural networks API, written in Python and'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='capable of running on top of  TensorFlow , CNTK , or Theano . It \\nwas developed with a focus on enabling fast experimentation.  https://keras.io  \\nScikit -Learn85 Scikit -learn is a free software machine learning library for the \\nPython programming language.  https://scikit -\\nlearn.org/stable/  \\n  \\n \\n4. GENERATIVE CHEMISTRY  WITH THE RECURRENT NEURAL NETWORK (RNN)  \\nRNN is a widely used neural network archi tecture in generative chemistry for  proposing  novel structure s. As a'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='type of powerful generative model especially in na tural language processing, RNNs usually use sequences of \\nwords, strings , or letters as the input and output44, 86-88. In this case, the SMILES strings are usually employed as \\na molecular repr esentation. Different from ANNs and CNNs which do not have m emories, RNNs iterative ly \\nprocess sequences and store  a state holding current information. On the contrary, ANNs and CNNs process each'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 9}, page_content='input independently without stored information between them. When describing an RNN, it can be considered \\nas a network with an internal loop that loops over the sequence elements instead of processing in a single step \\n(Fig. 2a ). The state that stored information will be updated during each loop.  For simplicity, the process of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='11 \\n computing the output y can follow the equation:  y = activation  (W ox + U oh + b o), where Wo and Uo are weight \\nmatrices for the input x and state h, and bo as a bias vector. Figure 2a  can represent the structure of a simple \\nRNN model. However, this  structure can suffer severely from the vanishing gradient problem which makes \\nneural networks untrainable after adding more layers. Even though the state h is supposed to hold the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='information from the sequence elements previously seen, the long -term depen dencies make the learning process \\nimpossible89, 90. The Long Short -Term Memory (LSTM) algorithm45 was developed to overcome this \\nshortcoming. The LSTM layer attaches a carry track to carry information across the learning process  to counter \\nthe loss of signals f rom gradual  vanishing  (Fig. 2b ). With this carry track, the information learned from each'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='sequence element can be loaded , and the loaded information can be transported and accessed at a later stage. \\nThe process of computing the output y for LSTM is similar with the previous equation but adding  the \\ncontribution of the carry track: y = activation  (W ox + U oh + Voc + bo), where Wo, Uo, and Vo are weight \\nmatrices for the input x, state h, and carry c, and bo as a bias vector. In certain cases, multiple recurrent layers in'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='a model can be stacked to e nhance representational power.  \\nA typical framework on generative modeling for molecule  generation applying LSTM algorithm ( Fig. \\n2c) starts with the collection of training molecules. The RNN model can be fine -tuned through the transfer \\nlearning that first accumulates knowledge from the large compound datasets and then produces the novel \\nstructures by learning smaller focused datasets. When  the collections of training molecules (for large sets or'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='small focused sets) are ready, SMILES strings can be calculate d for each molecule. One -hot encoding is a \\nregular operation for processing the molecular representations. In one -hot encoding, a unique integer index i is \\nassigned to every character in the SMILES string. Then a binary vector can be constructed of size C (the \\nnumber of unique characters in the string) with all zeros but for the ith entry which is one. For instance, there'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='are four ( C = 4) unique characters, “C”, “N”, “c”, and “1” in SMILES strings, input “C” is transferred  to (1, 0, 0, \\n0), “N” to (0, 1, 0, 0), “c” to (0, 0, 1, 0), and “1” to (0, 0, 0, 1) after one -hot encoding. In practice, usually an \\nadditional starting character like “G” and an ending character like “E” will be added to the SMILES to denote a \\ncomplete string. The neural network with LSTM l ayer(s) can be trained to predict the n+1th character given the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='input of string with n characters. The probability of distribution for the n+1th character is calculated as the loss \\nto evaluate the model performance. With the trained model, the sampling pro cess can start with the starting \\ncharacter or certain SMILES strings of molecular fragments to sample the next character until the ending \\ncharacter is hi t. The SMILES strings are reversed from the generated binary matrices according to the previous'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 10}, page_content='one-hot encoding to construct the molecular graphs as the output for this gen erative model.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 11}, page_content='12 \\n \\n \\nFigure 2 . The RNN, the LSTM, and their application in generative chemistry. a. The s chematic illustration of the \\nRNN, the neural network with an internal loop. b. The schematic illustration of data processing with the LSTM. c. \\nThe typical framework on building generative models applying RNN for molecule s generation.  \\n \\nRepresentative c ase studies are discussed in this paragraph. All the case applications covered in this'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 11}, page_content='review are summarized in Table 4 . Anvita Gupta et al. trained an LSTM -based generative model with transfer \\nlearning to generate libraries of molecules with structural similarity to known actives f or PPAR\\uf067 and trypsin91. \\nThe model was first trained with 550,000 SMILES strings of active compo unds from ChEMBL and further \\nfine-tuned with SMILES strings for 4,367 PPAR \\uf067 ligands and 1 ,490 trypsin inhibitors. Among the valid'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 11}, page_content='generated molecules, around 90% are unique from the know n ligands and are different from each other. The \\nproposed model was as sessed for fragment -based drug discovery as well. In fragmen t-based drug discovery, \\nfragment  growing is a strategy for novel compounds generation with the identified fragment lead. Substitutions \\ncan be added to the identified fragment with the consideratio n of pharmacophore features and proper physical -'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 11}, page_content='chemical properties to enhance the receptor -ligand interactions92. Instead of using the starting character to'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='13 \\n initiate the generative process, the SMILES string of the molecular fragment can be read and extended by \\ncalculating the probability of distribution for the next character. Marwin H. S. Segler et al. also reported their \\napplication of LSTM -based generative models for structure generation with transfer learning93. There was a \\ngood correlation between the generated structures and the molecules used for training. Notably , the complete de'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='novo  drug design cycle can be achieved with target prediction models for scoring. As the target prediction \\nmodel can be a molecu lar docking algorithm or even robot synthesis and bio -testing system, the drug design \\ncycle does not require known active compounds to start. Chemical Language Model (CLM) proposed by \\nMichael Moret et al. is another example of applying LSTM -based generativ e models to work with chemical'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='SMILES strings with transfer learning processes94. This approach enables the early stage molecular design in a \\nlow data regime. When it comes to real -world validation, Daniel Merk et al. published their prospective study \\nwith experimental evaluations95. Using the SMILES strings as the input, the LSTM -based generative model was \\ntrained and fine -tuned with the transfe r learning process for the peroxisome proliferator -activated receptor . Five'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='top-ranked compounds designed by the model were synthesized and tested. Four of them have nanomolar to \\nlow micromolar activities  in cell -based assays. Besides using the LSTM algori thm, some other RNN \\narchitectures such as implementing Gated Recurrent Unit96 (GRU) can also have promising applications. GRU \\nlayers work with the same principle as LSTM layers but may have less representational power. Shuangjia'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='Zheng et al. developed a quasi -biogenic molecular generator with GRU layers97. As biogenic compounds and \\npharmaceutical agents are biologically relevant, ov er 50% of existing drugs re sult from drug discovery \\ncampaigns starting with biogenic molecules. Their generative model is an effort to explore greater biogenic \\ndiversity space. Similarly, focused compound libraries can be constructed with transfer learning processes.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='5. GENERATIVE  CHEMISTRY WITH THE VARIATIONAL AUTOENCODER  (VAE)  \\nThe principle aim of an autoencoder (AE) is to construct a low -dimensional latent space of compressed \\nrepresentations that each element can be reconstructed to the original input  (Fig. 3a ). The module that maps the \\noriginal input data, which is in high -dimension, to a low -dimensional representation is called t he encoder, while'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='the module that realizes the mapping and reconstructs the original input from the low -dimensional \\nrepresentation is called the decode r41, 98. The encoder and the decoder are usually neural networks with RNN \\nand CNN architectures as SMIL ES strings and molecular graphs are commonly used molecular representations. \\nWith the molecular representations calculated, a  typical data processing procedure with AE on molecule'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='generation starts with encoding the input into a low -dimensional latent spac e. Within the latent space, the axis \\nof variations from the  input  can be encoded . Using the variation of molecular weight (M.W.) as an example, \\nwhile in practice the features learned can be highly abstractive as the M.W. is used here for simplified \\nillustr ation, the points along t his axis  are embedded representations of compounds with different M.W. These'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 12}, page_content='variations are termed concept vectors. With an identified vector, it makes the molecular editing possible by'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 13}, page_content='14 \\n exploring the representations in a relevant d irection. The encoded latent space with compressed representations \\ncan then be sampled with the decoder to map them back to m olecular representations . Novel structures \\nalongside the original input can be  expected.  \\n \\n \\nFigure 3. The autoencoder and the variational autoencoder. a . An autoencoder encodes input molecules into \\ncompressed representations and decodes them back . b. A variational autoencoder maps the molecules into the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 13}, page_content='parameters of a statistical distribution  as the latent space is a continuous nume rical representation.  \\n \\nThe concept of VAE was first proposed by Kingma and Welling at the end of 201399, 100. This technique \\nquickly gained  popularity in building robust generative models for images, sounds, and texts101-103. The AE \\ncompresses a molecule x into a fixed code in the continuous latent space z, and trends to summarize the explicit'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 13}, page_content='mapping rules as the number of adjustable parameters is often much  more than the number of training \\nmolecules. These explicit rules make t he decoding of random  points in the continuous latent space challenging \\nand sometimes impossible26. Instead, VAE maps the molecules into the parameters of a statistical distribution \\n(Fig. 3b ). With p(z) describing  the distribution of prior continuous latent space, the probabilistic encoding'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='15 \\n distributi on is q\\uf06a(z|x) and the probabilistic decoding distribution is p\\uf071(x|z). The training iterations with back  \\npropagation will gradually optimize the parameters of both q\\uf06a(z|x) and p\\uf071(x|z). VAE is  fundamentally a latent \\nvariable mode l p(x,z ) = p\\uf071(x|z)p(z). The s tochasticity of the training process enables the latent space to encode \\nvalid representations, which further results in a structured latent space100. Both the reconstruction loss and the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='regularization loss are often used for paramet er optimization during the training process. The reconstruction loss \\nevaluates whether the decoded samples match the input while the regularization loss investigates whether the \\nlatent space is overfitting to the training data.  \\n Applications of VAE for ge nerating chemical structures  started in 2016 as Rafael  Goḿez -Bombarelli et \\nal. developed a VAE -based automatic chemical design system104. In their practice, the ZINC database and QM9'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='dataset were referred to as the sources for collecting molecules.  The QM9 dataset archives small molecules \\nfollowing three rules: (1) no  more than 9 heavy atoms, (2) with 4 distinct atomic numbers, and (3) with 4 bond \\ntypes. Canonical SMILES strings were calculated as the molecular representation. The encoder maps input \\nSMILES strings into the continuous real -valued vector s, and the decode r reconstructs  molecular representations'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='from the se vector s. The e ncoder was formed with three convolutional layers and one fully connected dense \\nlayer while the decoder contained three GRU layers. The architectures of CNNs and RNNs were compared for \\nstring encoding and convolutional layers achieved superior performance. The last layer of the decoder would \\nreport a probability distr ibution for characters of the S MILES string at each position. This stochastic operation'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='allowed the same point in the latent sp ace to have different decoded outcomes. Besides, they added one \\nadditional module for property prediction. An MLP was jointed to predict the property values from the \\ncontinuous representation created by the encoder in order to optimize the desired properti es for the new \\nmolecules. Thomas Blaschke et al. tested various generative AE models including VAE for compound design'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='targeting dopamine receptor 2  (DRD2)105. Their study showed that the generated latent space preserved the \\nchemical similarity principles. The generated molecules similar to known acti ve compounds can be observed. In \\ntheir VAE model, CNN layers were used for the encoder for pattern recognition and the RNN layers of GRU \\ncells were adapted for the decoder. The ChEMBL database functioned as the data source for molecular'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='structures. Canonic al SMILES were prepared as the molecular representation. Similarly, a n SVM classification \\nmodel trained with extensive circular fingerprint (ECFP) of active and inactive DRD2 ligands was integrated to \\ninvestigate the newly generated molecules.  Boris Sattar ov et al. combined a sequence -to-sequence V AE model \\nwith generative topographic mapping (GTM) for molecular design106. Both the encoder and the decoder were'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 14}, page_content='RNN models c ontaining two LSTM layers in their practice. SMILES strings with one -hot encoding for \\nmolecules from  the ChEMBL database were prepared prior to the training. Their GTM module contributed to \\nthe selection  of sampling points in the VAE latent space, which fa cilitated the generation of a focused library of \\ncompounds with desired properties.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='16 \\n Besides the use of SMILES strings, molecular graphs have also been applied as a type of molecular \\nrepresentation  to feed the VA E models. Bidisha Samanta et al. proposed Ne VAE , a VAE -based  compound  \\ngenerative model employing  molecular graphs107. The molecular structures  are usually not grid -like and come \\nwith an inconsistent number of nodes and edges, which impedes the use of molecular graphs as representations.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='In their wo rk, the molecular graphs were prepared for  drug-like compounds  collected from the ZINC database \\nand QM9 dataset. The nodes and edges in the graph represent atoms and bonds respectively. The node features \\nare types of atoms with one -hot encoding and the edge weights are bond types (saturated bonds, unsaturated \\ndouble/tripl e bonds, etc.) . The purpose of training is to enable the VAE to create credible molecular graphs'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='including node features and edge weights. Another example is GraphVAE . Martin Simonovsky et al. proposed \\nGraphVAE  to facilitate the compound design using  molec ular graphs108. Their central hypothesis was to decode \\na probabilistic fully -connected graph in which the existence of nodes, edges, and their attributes are independent \\nrandom variables.  The encoder was a feed -forward network with convolutional layers  and the architecture for'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='the decoder was an MLP. The model training and evaluation involved the molecules from the ZINC database \\nand QM9 dataset.  Some other generative applications can switc h the topic to lead optimizations with methods \\nsuch as scaffold hopping, substitutions design, and fragment -based approaches. One example is the DeLinker \\nwhich was  proposed by Fergus Imrie et al. to incorporate two fragments into a new molecule109. This method is'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='VAE -based, using molecular graphs as the i nput. The design process heavily relied  on 3D structural information \\nthat considers relative distance and orientation between the starting fragments.  \\n \\n6. GENERATIVE CHEMISTRY WITH THE ADVERSARIAL AUTOENCODER  (AAE)  \\nThe architecture of the AAE is comparativ ely similar to the VAE except the appending of the additional \\ndiscriminator network110. An AAE trains three modules, an encoder, a decoder, and a discr iminator  (Fig. 4).'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='The encoder learns the input data and maps the molecule into the latent space following the distribution of \\nq\\uf06a(z|x). The decoder reconstructs molecules through sampling from the latent space following the probabilistic \\ndecoding distribut ion of p\\uf071(x|z). And the discriminator distinguishes the distribution of the latent space z ~ q\\uf06a(z) \\nfrom the prior distribution z’ ~ p(z). During the training iterations, the encoder is modified consistently to have'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 15}, page_content='the output, q\\uf06a(z|x), follow a specific di stribution, p(z), in an effort to minimize the adver sarial cost of the \\ndiscriminator. A simplistic prior, like Gaussian distribution, is assumed in VAE, while alternative priors can \\nexist in real -world practices111. The AAE  architecture  with the additional discriminator  module demonstrates \\nimproved adaptability .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='17 \\n \\n \\nFigure 4. The illustrated a rchitecture of an adversarial autoencoder. A discriminator network is appended to \\ncalculate the adversarial cost for discriminating p(z) from q\\uf06a(z). As a result, the outcome latent space from the \\nencoder is driven to follow the prior distribution.  \\n \\nThomas Blaschke et al.  summarized a three -step training process in their compound design practice with'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='AAE: (1) The simultaneous training of  both the encoder and the decoder to curtail  the reconstruction loss  of the \\ndecoder; (2) The training of the discriminator to distinguish the distribution of the latent space, q\\uf06a(z), from the \\nprior distribution p(z) effectively; (3) The training of the enc oder to minimize the adversarial cost for \\ndiscriminating p(z) from q\\uf06a(z)105. The training iterations continue until the reconstruction loss converges. Artur'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='Kadurin et al. proposed the method of using  a generative adversarial autoencoder model to identify fingerprints \\nof new molecules with potential a nticancer properties111. The input molecules come from a  small data set of \\ncompounds profiled on the MCF -7 cell line . The MACCS fingerprints were used as the molecular \\nrepresentation and two fully connected dense layers with different dimensions were used as the network'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='architecture for the encoder, decoder, and the discriminator.  One notable modification in this study was the \\nremoval of the batch normalization layers for the discriminator. Batch normalization is an optimiza tion method \\nthat reduces the covariance shift among the hidden units and allows each layer to learn more independently. In \\nthe authors’ opinion, the noise from the generator can be masked into target random noise with the batch'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='normalization layers, which prohibits the training of the discriminator. As each bit of the MACCS fingerprints \\nrepresents certain substructure features, the learned structural information by machine can be beneficial to the \\ndesign of chemical derivatives for identified leads. Daniil Polykovskiy et al. reported their work on building a \\nconditional AAE for molecule design targeting Janus kinase 3  (JAK3)112. The contributions from  a set of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 16}, page_content='physical -chemical properties including bioactivity, solubility, and synthesizability were considered and the \\nmodel was conditioned to produce molecules with specified properties. Clean lead molecules were c ollected \\nfrom the ZINC database and encoded as SMILES strings. The LSTM layers are adapted for building the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='18 \\n encoder and the decoder networks. Both in silico  method (molecular docking) and in vitro  assay ( inhibition of \\nJAK2 and JAK3 ) were conducted as the e valuation for the newly generated molecules. Rim Shayakhmetov et al. \\nreported a bidirectional AAE  model that generates molecules with the capacity of inducing a desired change in \\ngene expression113. The model was va lidated using LINCS L1000, a database that collects gene expression'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='profiles. The molecular structures x and induced gene expression changes y contributed to a joint model p(x,y) . \\nIn this specific conditional task, there is no direct association between x and y as certain changes at the gene \\nexpression are irrelevant to the drug -target interactions. The proposed bidirectional AAE model then learned  the \\njoint distribution and decomposed objects into shared features, exclusive features to  x, and exclusive fea tures to'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='y. Therefore, t he discrimin ator that divides  the latent representations into sh ared and exclusive sections was  \\nconstructed to secure the conditional generation to be consequential.  \\n \\nTable 4 . Representative applications of generative chemistry cov ered in this review   \\n# Generative \\narchitecture  Neural networks \\ninvolved  Data \\nsource  Molecular \\nrepresentation  Note  Ref. \\n1 RNN  LSTM  ChEMBL  SMILES  The application was extended to \\nfragment -based drug design . 91'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='fragment -based drug design . 91 \\n2 RNN  LSTM  ChEMBL  SMILES  The d esign -synthesis -test cycle was \\nsimulated with tar get prediction models \\nfor scoring.  93 \\n3 RNN  LSTM  ChEMBL  SMILES  A chemical language model (CLM) in low \\ndata regimes.  94 \\n4 RNN  LSTM  ChEMBL  SMILES  A prospective application with \\nexperimental validations of top -ranking \\ncompounds.  95 \\n5 RNN  GRU  ZINC  \\nChEMBL  SMILES  The generative model explored greater \\nbiogenic diversity space.  97 \\n6 VAE  Encoder: CNN'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='6 VAE  Encoder: CNN  \\nDecoder: GRU  ZINC \\nQM9  SMILES  An MLP model was jointed to predict \\nproperty values.  104 \\n7 VAE  Encoder: CNN  \\nDecoder: GRU  ChEMBL  SMILES  An SVM classification model was added \\nto evaluate the outco me. 105 \\n8 VAE  Encoder: LSTM  \\nDecoder: LSTM  ChEMBL  SMILES  A sequence -to-sequence VAE model was \\ncombined with generative topographic \\nmapping (GTM) for molecular design.  106 \\n9 VAE  Encoder: CNN  \\nDecoder: CNN  ZINC  \\nQM9  Molecular'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 17}, page_content='Decoder: CNN  ZINC  \\nQM9  Molecular \\ngraph  The nodes and edges in the graph of \\nNeVAE represent atoms and bonds \\nrespectively.  107 \\n10 VAE  Encoder: CNN  \\nDecoder: MLP  ZINC  \\nQM9  Molecular \\ngraph  The central hypothesis of GraphVAE was \\nto decode a probabilistic fully -connected \\ngraph . 108'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='19 \\n 11 VAE  Encoder: GGNN# \\nDecoder: GGNN  ZINC  \\nCASF* Molecular \\ngraph  DeLinker was designed to incorporate two \\nfragments into a new molecule.  109 \\n12 AAE  Encoder: MLP  \\nDecoder: MLP  \\nDiscriminator: MLP  MCF -7^ MACCS \\nfingerprints  Fingerprints cannot be directly converted \\nto structures but can provide certain \\nsubstruct ure information.  111 \\n13 AAE  Encoder: LSTM  \\nDecoder: LSTM  \\nDiscriminator: MLP  ZINC  SMILES  The generated molecules targeting JAK3'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='were evaluated with in silico  and in vitro  \\nmethods.  112 \\n14 AAE  Encoder: GRU  \\nDecoder: GRU  \\nDiscriminator: MLP  LINCS& \\nChEMBL  SMILES  The combination of molecules and gene \\nexpression data were analyzed.  113 \\n15 GAN  Discriminator: CNN  \\nGenerator: LSTM  ZINC  SMILES  Sequence generation with objective -\\nreinforced generative adversarial networks \\n(ORGAN) . 114 \\n16 GAN  Discriminator: MLP  \\nGenerator: MLP  ZINC  Molecular \\ngraph  The model operated in the latent space'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='graph  The model operated in the latent space \\ntrained by the Ju nction Tree VAE . 115 \\n17 GAN  Discriminator: MLP  \\nGenerator: MLP  LINCS& SMILES  The compound design was connected to \\nthe systems biology.  116 \\n18 GAN  Encoder: LSTM  \\nDecoder: LSTM  \\nDiscriminator: MLP  \\nGenerator: MLP  ChEMBL  SMILES  The concept of the autoencoder and the \\ngenerative adversarial network was \\ncombined to propose a latentGAN.  117'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='combined to propose a latentGAN.  117 \\n#GGNN represent s the gated graph neural network. *CASF is also known as PDBbind core set.  ^MCF -7 represents a small \\ndata set of compounds profiled on the MCF -7 cell line . &LINCS represents the LINCS L1000 dataset that collects gene \\nexpression profiles.  \\n \\n7. GENERATIVE CHEMISTRY WITH THE GENERATIVE ADVERSARIAL NETWORK (GAN)  \\nThe architecture of the convolutional neural netwo rk43 (CNN) is  briefly covered in this section as the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='convolutional layers are widely used i n GAN mod eling. The implementation  of convolutional layers can also be \\nfound  in case studies discussed above among autoencoder models . A con volutional layer does not learn  an input  \\nglobally  but focus es on the local pattern within a receptive field , the kernel  (Fig. 5a). The low -level patterns \\nlearned in a prior layer can then be concentrated on the high-level features at the subsequent layers118, 119. This'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='characteristic allows the CNN to learn and summarize  abstract patterns with complexity. Another characteristic  \\nthat comes out from the  local pattern learning is that the learned features can be recognized anywhere118. It \\nenables the CNN to process input data with  efficiency  and powerful ness even with a  smaller number of input \\nsample  representations. Meanwhile, multiple feature maps (filters) can be stacked to encode diffe rent a spects of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 18}, page_content='the input data. Applying several  filters capacitates  a CNN model to detect distinct  features anywhere  among the \\ninput data. The pooling operation on the other hand subsamples the feature map  to reduce the number of \\nparameters and eventually, the computational load120. Using a max -pooling layer as one example, only the max \\ninput value in that pooling kernel will be k ept. Alongside with dropout layers and regularization p enalties, the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 19}, page_content='20 \\n pooling layers also contribute  to confronting the overfitting issues. Putting together, the convolutional layers, \\npooling layers, and dense layers are carefully selected and arranged to construct a sophisticated CNN \\narchitecture.  \\n \\n \\nFigure 5. Sample architecture  of the convolutional neural network and the framework  of a generative adversarial \\nnetwork . a. The careful selection and arrangement of convolutional layers, pooling layers, and dense layers , etc.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 19}, page_content='constitute a convolutional neural netw ork. b . The generative adversarial network comprises  two modules, the \\ngenerator and the discriminator.  Both the generative loss and discriminative  loss are monitored during the \\ntraining process.  \\n \\n The concept of the GAN was first raised  by Ian Goodfellow  in 201446. The method quickly gained \\npopularity on generative tasks regarding image, video, and audio processing and related areas121-123. Two'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 19}, page_content='models, the discriminato r and the generator are trained  iteratively and simultaneously during the adver sarial \\ntraining process63. The discriminator is supposed to discover the hidden patterns behind the input data and to \\nmake accurate discrimination of the authentic data from the ones generated by the generator. The generator is \\ntrained to keep proposing compelling data t o fool the well -trained discriminator by consistently optimizing the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 19}, page_content='data sampling process. The training process is a zero -sum noncooperative game with the purpose of achieving'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='21 \\n the Nash equilibrium by the discriminator and the generator. In generative chem istry, the g enerator generates \\nSMILES strings, molecular graphs, or fingerprints, depending  on the selection of the molecular representation, \\nusing the latent random inputs ( Fig. 5b ). The generated molecules are mixed with the samples of real \\ncompounds to feed the discriminator after correct labeling. The discriminative loss is calculated to evaluate'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='whether the discriminator can distinguish the real compounds from the generated ones, while the generative loss \\nis computed to assess whether the generator can  fool the discriminator by generating undistinguishable \\nmolecules. The constringency of both loss functions after the iterative training indicates that even a well -\\nestablished discriminator can be misled to classify generated molecules as real, which furth er reflects that the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='generator has learned and accumulated authentic data patterns to create captivating compounds. However, it is \\nworth mentioning that the simultaneous optimization of both loss functions is challenging as the instability can \\nlead to the gradient of one part instead of both being favored (results in a stronger discriminator or generator, \\nbut not both).  Another limitation may come from the restricted chemical space that is being covered by the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='generated molecules64. To confront the discriminator and minimize the generative  loss, the generator can only \\nexplore a limited chemical space defined by the real compounds.  \\n Gabriel Guimaraes et al. presented  a seq uence -based GAN framework termed objective -reinforced \\ngenerative adversarial network (ORGAN)114 that includes domain -specific objectives to the training process \\nbesides the discriminator reward. The discr iminator drove the generated samples to follow the distribution of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='the real data and the domain -specific objectives secured that the traits maximizing the specific heuristic would \\nbe selected. The d rug-like and nondrug -like molecules were collected from ZI NC databases. SMILES st rings \\nwere calculated  as the molecular  representations. A CNN model was designed as the discriminator to classify \\ntexts, and a n RNN model with LSTM units was used as the generator. Łukasz Maziarka et al. introduced Mol -'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='CycleGAN for derivatives design and compound optimization115. The model could generate structures with high \\nsimilarity to the original input but improved values on considered properties. Molecular graphs of compounds \\nextracted from  the ZINC database were used as the molecular representation. The model operated in the latent \\nspace trained by the Junction Tree VAE. Dense layers and fully connected residual layers constituted the'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='generator and the discriminator. Oscar Méndez -Lucio et al.  reported a GAN model to connect  the compound \\ndesign with systems biology116. They have shown that active -like molecules can be generated given that the \\ngene expression signature of the selected target is supplied . The architectures  of both the discriminator  and the \\ngenerator were composed with dense layers. There were two stages of training: in stage I, the random noise was'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 20}, page_content='taken as the input, while in stage II, the output from stage I and the gene expression signature were taken.  \\nOleksii Prykhodko et al. combined the concept of AE with G AN and proposed a latent vector -based GAN \\nmodel117. A heteroencoder mapped one -hot encoded SMILES strings into the latent space and the generator and  \\ndiscriminator would directly use the latent vector to focus on the optimization of the sampling process. A pre -'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='22 \\n trained heteroencoder was then used to transfer the generated vectors back to molecular structures.  Both general \\ndrug-like compounds and target -biased molecules were generated as applications of the method.  \\n \\n8. CONCLUSION AND FUTURE PERSPECTIVES  \\nBesides the successful generative chemistry stories described above, challenges and opportunities can be found \\nat the following four aspects:  (1) the syn thetic feasibility of the generated structures, (2) the alternative'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='molecular representations that can better portray a structure, (3) the generation of macro -molecules, and ( 4) the \\nclose -loop automation in combination with experimental validations. Wenhao  Gao et al. pointed out that \\ngenerative models can propose unrealistic molecules even with high performance scores on quantitative \\nbenchmarks124. Some existing methods of evaluating the synthesizability are based on synthetic routes and'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='molecular structural data, which require heuristic definition to be complex and comprehensive125, while the \\nchange of on e single functional group to a scaffold can cause a  distinctive synthetic pathway.  The ignorance of \\nsynthesizabilit y turns out to be an eminent hindrance of connecting generative models with medicinal chemistry \\nsynthesis. The molecular representations such as SMILES st rings and molecular fingerprints serve well on'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='describing small molecules at the current stage. Howeve r, it will be appealing if the novel representations can \\nbe designed to also consider three -dimensional geometry data. Chiral compounds may exhibit divergent \\nactivities to the biological system126, and even th e conformational change of the same small molecule can alter \\nthe receptor -ligand interactions. The case studies that deployed molecular graphs as the representation illustrate'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='the benefits of working with structures directly107-109, 115. The extended consideration of bond type, length, and \\nangles improves  the performance of feature extraction on spatial patter ns. Peptides possess superior advantage \\namong protein subtype selectivity. The strategy of developing antibodies and peptides as therapeutic agents \\ndraw s increasing attention from both the academia and industry. Deep learning is  data-driven research. Curre nt'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='generative chemistry applications mainly focus on the design o f small molecules as there is  increased \\navailability of accessing chemical data127. As the construction of protein -related databases is rising, the attempts \\nof de novo protein generation are expected128. Better representations are certainly required for describing \\nprotein , as the folding and its conformation are even more critical to determine the functionality. Lastly, it is'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='noteworthy of how to integrate the generative chemistry into  the drug design framework to close the loop of this \\nautomated process. Marwin H. S. Segler  et al. mentioned a design -synthesis -test cycle in their application of \\nusing the RNN model to generate molecules93. Ideally, the HTS will first recognize some hit compounds for a \\ngiven target. The identified hits will contribute to the iterative training of a deep learning generative model for'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 21}, page_content='novel compounds generation,  and a machine learning -based target prediction model for virtual classification. \\nThe top molecules will be synthesized and tested with biological assays. The true new actives can then be \\nappended to the identified hits, which closes the loop.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='23 \\n  In a nutshe ll, this paper reviewed  the latest advances of generative chemistry that utilizes deep learning \\ngenerative models to expedite the drug discovery process. The review starts with a brief history of AI in drug \\ndiscovery to outline this emerging paradigm. Comm only used chemical databases, molecular representations, \\nand operating sources of cheminformatics and machine learning are covered as the infrastructure. The detailed'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='discussion s on RNN, VAE, AAE, and GAN are centered, which is followed by future perspecti ves. As a fast -\\ngrowing area of research, we are optimistic to expect a boosting number of studies on generative chemistry. We \\nare probably at the corner of an upcoming revolution of drug discovery in the AI era, and the good news is that \\nwe are witnessing the change.  \\n \\n9. AUTHOR INFORMATION  \\nCorresponding author  \\nAuthor to whom correspondence should be addressed: Xiang -Qun Xie  \\nNotes'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='Notes  \\nThe authors declare no competing financial interest.  \\n \\n10. ACKNOWLEDGEMENTS  \\nAuthors would like to acknowledge the funding support to the Xie laboratory from the NIH NIDA (P30 \\nDA035778A1) and DOD (W81XWH -16-1-0490).  \\n \\n11. REFERENCES  \\n1. Chan, H. S.; Shan, H.; Dahoun, T.; Vogel, H.; Yuan, S., Advancing drug discovery via artificial intelligence. \\nTrends in pharmacologi cal sciences 2019 .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='Trends in pharmacologi cal sciences 2019 . \\n2. Dickson, M.; Gagnon, J. P., The cost of new drug discovery and development. Discovery medicine 2009 , \\n4, 172 -179.  \\n3. Chen, H.; Engkvist, O.; Wang, Y.; Olivecrona, M.; Blaschke, T., The rise of deep learning in drug \\ndiscovery. Drug disc overy today 2018 , 23, 1241 -1250.  \\n4. Broach, J. R.; Thorner, J., High -throughput screening for drug discovery. Nature 1996 , 384, 14 -16.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='5. Kroemer, R. T., Structure -based drug design: docking and scoring. Current protein and peptide science \\n2007 , 8, 312 -328. \\n6. Blundell, T. L., Structure -based drug design. Nature 1996 , 384, 23.  \\n7. Bacilieri, M.; Moro, S., Ligand -based drug design methodologies in drug discovery process: an overview. \\nCurrent drug discovery technologies 2006 , 3, 155 -165.  \\n8. Pagadala, N. S.; Sy ed, K.; Tuszynski, J., Software for molecular docking: a review. Biophysical reviews \\n2017 , 9, 91 -102.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='2017 , 9, 91 -102.  \\n9. Bian, Y. -m.; He, X. -b.; Jing, Y. -k.; Wang, L. -r.; Wang, J. -m.; Xie, X. -Q., Computational systems \\npharmacology analysis of cannabidiol: a combination of  chemogenomics -knowledgebase network analysis and \\nintegrated in silico modeling and simulation. Acta Pharmacologica Sinica 2019 , 40, 374 -386.  \\n10. Bian, Y.; Feng, Z.; Yang, P.; Xie, X. -Q., Integrated in silico fragment -based drug design: case study with'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 22}, page_content='allosteric modulators on metabotropic glutamate receptor 5. The AAPS journal 2017 , 19, 1235 -1248.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='24 \\n 11. Wang, J.; Wolf, R. M.; Caldwell, J. W.; Kollman, P. A.; Case, D. A., Development and testing of a general \\namber force field. Journal of computational chemist ry 2004 , 25, 1157 -1174.  \\n12. Vanommeslaeghe, K.; Hatcher, E.; Acharya, C.; Kundu, S.; Zhong, S.; Shim, J.; Darian, E.; Guvench, O.; \\nLopes, P.; Vorobyov, I., CHARMM general force field: A force field for drug ‐like molecules compatible with'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='the CHARMM all ‐atom additive biological force fields. Journal of computational chemistry 2010 , 31, 671 -690.  \\n13. Ge, H.; Bian, Y.; He, X .; Xie, X. -Q.; Wang, J., Significantly different effects of tetrahydroberberrubine \\nenantiomers on dopamine D1/D2 receptors revealed by experimental study and integrated in silico simulation. \\nJournal of computer -aided molecular design 2019 , 33, 447 -459.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='14. Hajduk, P. J.; Greer, J., A decade of fragment -based drug design: strategic advances and lessons learned. \\nNature reviews Drug discovery 2007 , 6, 211 -219.  \\n15. Yang, S. -Y., Pharmacophore modeling and applications in drug discovery: challenges and recent \\nadvances. Drug discovery today 2010 , 15, 444 -450.  \\n16. Wieder, M.; Garon, A.; Perricone, U.; Boresch, S.; Seidel, T.; Almerico, A. M.; Langer, T., Common hits'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='approach: combining pharmacophore modeling and molecular dynamics simulations. Journal of chemical \\ninformation and modeling 2017 , 57, 365 -385.  \\n17. Liu, Z.; Chen, H.; Wang, P.; Li, Y.; Wold, E. A.; Leonard, P. G.; Joseph, S.; Brasier, A. R.; Tian, B.; Zhou, J., \\nDiscovery of Orally Bioavailable Chromone Derivatives as Potent and Selective BRD4 Inhibitors: S caffolding \\nHopping, Optimization and Pharmacological Evaluation. Journal of Medicinal Chemistry 2020 .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='18. Hu, Y.; Stumpfe, D.; Bajorath, J. r., Recent advances in scaffold hopping: miniperspective. Journal of \\nmedicinal chemistry 2017 , 60, 1238 -1246.  \\n19. Muegge, I.; Mukherjee, P., An overview of molecular fingerprint similarity search in virtual screening. \\nExpert opinion on drug discovery 2016 , 11, 137 -148.  \\n20. Fan, Y.; Zhang, Y.; Hua, Y.; Wang, Y.; Zhu, L.; Zhao, J.; Yang, Y.; Chen, X.; Lu, S.; Lu, T., Inve stigation of'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='Machine Intelligence in Compound Cell Activity Classification. Molecular Pharmaceutics 2019 , 16, 4472 -4484.  \\n21. Minerali, E.; Foil, D. H.; Zorn, K. M.; Lane, T. R.; Ekins, S., Comparing Machine Learning Algorithms for \\nPredicting Drug -Induced L iver Injury (DILI). Molecular Pharmaceutics 2020 . \\n22. Karras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; Aila, T., Analyzing and improving the image \\nquality of stylegan. arXiv preprint arXiv:1912.04958 2019 .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='23. Wen, T. -H.; Gasic, M.; Mrksic, N.; Su, P. -H.; Vandyke, D.; Young, S., Semantically conditioned lstm -based \\nnatural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745 2015 . \\n24. Zhavoronkov, A.; Ivanenkov, Y. A.; Aliper, A.; Veselov, M. S.; Aladinskiy, V. A.; Aladinskaya, A. V.; \\nTerentiev, V. A.; Polykovskiy, D. A.; Kuznetsov, M. D.; Asadulaev, A., Deep learning enables rapid identification'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='of potent DDR1 kinase inhibitors. Nature biotechnology 2019 , 37, 1038 -1040.  \\n25. Turing, A. M. Computing machinery and inte lligence. In Parsing the Turing Test ; Springer: 2009, pp 23 -\\n65. \\n26. Chollet, F., Deep Learning mit Python und Keras: Das Praxis -Handbuch vom Entwickler der Keras -\\nBibliothek . MITP -Verlags GmbH & Co. KG: 2018.  \\n27. Segler, M. H.; Preuss, M.; Waller, M. P., Pl anning chemical syntheses with deep neural networks and \\nsymbolic AI. Nature 2018 , 555, 604 -610.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='symbolic AI. Nature 2018 , 555, 604 -610.  \\n28. Lipinski, C. A., Rule of five in 2015 and beyond: Target and ligand structural limitations, ligand \\nchemistry structure and drug discovery project decisions.  Advanced drug delivery reviews 2016 , 101, 34 -41. \\n29. Bian, Y.; Jing, Y.; Wang, L.; Ma, S.; Jun, J. J.; Xie, X. -Q., Prediction of orthosteric and allosteric \\nregulations on cannabinoid receptors using supervised machine learning classifiers. Molecular pharm aceutics'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 23}, page_content='2019 , 16, 2605 -2615.  \\n30. Lo, Y. -C.; Rensi, S. E.; Torng, W.; Altman, R. B., Machine learning in chemoinformatics and drug \\ndiscovery. Drug discovery today 2018 , 23, 1538 -1546.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='25 \\n 31. Jing, Y.; Bian, Y.; Hu, Z.; Wang, L.; Xie, X. -Q. S., Deep learning f or drug design: an artificial intelligence \\nparadigm for drug discovery in the big data era. The AAPS journal 2018 , 20, 58.  \\n32. Bzdok, D.; Altman, N.; Krzywinski, M., In; Nature Publishing Group: 2018.  \\n33. Vamathevan, J.; Clark, D.; Czodrowski, P.; Dunham, I.; Ferran, E.; Lee, G.; Li, B.; Madabhushi, A.; Shah,'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='P.; Spitzer, M., Applications of machine learning in drug discovery and development. Nature Reviews Drug \\nDiscovery 2019 , 18, 463 -477.  \\n34. Korotcov, A.; Tkachenko, V.; Russo, D. P.; Ekins, S., Compariso n of deep learning with multiple machine \\nlearning methods and metrics using diverse drug discovery data sets. Molecular pharmaceutics 2017 , 14, \\n4462 -4475.  \\n35. Ma, X. H.; Jia, J.; Zhu, F.; Xue, Y.; Li, Z. R.; Chen, Y. Z., Comparative analysis of machine lea rning'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='methods in ligand -based virtual screening of large compound libraries. Combinatorial chemistry & high \\nthroughput screening 2009 , 12, 344 -357.  \\n36. Verma, J.; Khedkar, V. M.; Coutinho, E. C., 3D -QSAR in drug design -a review. Current topics in medicinal  \\nchemistry 2010 , 10, 95 -115.  \\n37. Fan, F.; Warshaviak, D. T.; Hamadeh, H. K.; Dunn, R. T., The integration of pharmacophore -based 3D'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='QSAR modeling and virtual screening in safety profiling: A case study to identify antagonistic activities against \\nadenosine receptor, A2A, using 1,897 known drugs. PloS one 2019 , 14.  \\n38. Gladysz, R.; Dos Santos, F. M.; Langenaeker, W.; Thijs, G.; Augustyns, K.; De Winter, H., Spectrophores \\nas one -dimensional descriptors calculated from three -dimensional atomic properties: appli cations ranging'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='from scaffold hopping to multi -target virtual screening. Journal of cheminformatics 2018 , 10, 9.  \\n39. Nguyen, T. T.; Nguyen, N. D.; Nahavandi, S., Deep reinforcement learning for multiagent systems: A \\nreview of challenges, solutions, and app lications. IEEE Transactions on Cybernetics 2020 . \\n40. LeCun, Y.; Bengio, Y.; Hinton, G., Deep learning. nature 2015 , 521, 436 -444.  \\n41. Goodfellow, I.; Bengio, Y.; Courville, A., Deep learning . MIT press: 2016.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='42. Kleene, S. C. Representation of events in nerve nets and finite automata ; RAND PROJECT AIR FORCE \\nSANTA MONICA CA: 1951.  \\n43. LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P., Gradient -based learning applied to document recognition. \\nProceedings of the IEEE 1998 , 86, 2278 -2324.  \\n44. Rumelhart, D. E.; Hi nton, G. E.; Williams, R. J., Learning representations by back -propagating errors. \\nnature 1986 , 323, 533 -536.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='nature 1986 , 323, 533 -536.  \\n45. Hochreiter, S.; Schmidhuber, J., Long short -term memory. Neural computation 1997 , 9, 1735 -1780.  \\n46. Goodfellow, I.; Pouget -Abadie, J.; Mirza, M.; Xu, B.; Warde -Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. \\nGenerative adversarial nets. In Advances in neural information processing systems, 2014; 2014; pp 2672 -2680.  \\n47. Berman, H. M.; Westbrook, J.; Feng, Z.; Gilliland, G.; Bhat, T. N.; Weissig,  H.; Shindyalov, I. N.; Bourne, P.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='E., The protein data bank. Nucleic acids research 2000 , 28, 235 -242.  \\n48. UniProt: the universal protein knowledgebase. Nucleic acids research 2017 , 45, D158 -D169.  \\n49. Kim, S.; Thiessen, P. A.; Bolton, E. E.; Chen, J.; Fu,  G.; Gindulyte, A.; Han, L.; He, J.; He, S.; Shoemaker, B. \\nA., PubChem substance and compound databases. Nucleic acids research 2016 , 44, D1202 -D1213.  \\n50. Gaulton, A.; Hersey, A.; Nowotka, M.; Bento, A. P.; Chambers, J.; Mendez, D.; Mutowo, P.; Atkinson, F .;'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='Bellis, L. J.; Cibrián -Uhalte, E., The ChEMBL database in 2017. Nucleic acids research 2017 , 45, D945 -D954.  \\n51. Wishart, D. S.; Feunang, Y. D.; Guo, A. C.; Lo, E. J.; Marcu, A.; Grant, J. R.; Sajed, T.; Johnson, D.; Li, C.; \\nSayeeda, Z., DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic acids research 2018 , 46, \\nD1074 -D1082.  \\n52. Irwin, J. J.; Shoichet, B. K., ZINC− a free database of commercially available compounds for virtual'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 24}, page_content='screening. Journal of chemical information and modeling 2005, 45, 177 -182.  \\n53. Huang, Z.; Mou, L.; Shen, Q.; Lu, S.; Li, C.; Liu, X.; Wang, G.; Li, S.; Geng, L.; Liu, Y., ASD v2. 0: updated \\ncontent and novel features focusing on allosteric regulation. Nucleic acids research 2014 , 42, D510 -D516.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='26 \\n 54. Nidhi; Glick,  M.; Davies, J. W.; Jenkins, J. L., Prediction of biological targets for compounds using \\nmultiple -category Bayesian models trained on chemogenomics databases. Journal of chemical information and \\nmodeling 2006 , 46, 1124 -1133.  \\n55. Wang, L.; Ma, C.; Wipf, P.;  Liu, H.; Su, W.; Xie, X. -Q., TargetHunter: an in silico target identification tool \\nfor predicting therapeutic potential of small organic molecules based on chemogenomic database. The AAPS'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='journal 2013 , 15, 395 -406.  \\n56. Weininger, D., SMILES, a chemical la nguage and information system. 1. Introduction to methodology \\nand encoding rules. Journal of chemical information and computer sciences 1988 , 28, 31 -36. \\n57. OEChem, T., OpenEye Scientific Software. Inc., Santa Fe, NM, USA 2012 . \\n58. Landrum, G., RDKit: Open -source cheminformatics. 2006 . \\n59. O’Boyle, N. M., Towards a Universal SMILES representation -A standard method to generate canonical'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='SMILES based on the InChI. Journal of cheminformatics 2012 , 4, 22.  \\n60. Durant, J. L.; Leland, B. A.; Henry, D. R.; Nourse, J. G., Reoptimization of MDL keys for use in drug \\ndiscovery. Journal of chemical information and computer sciences 2002 , 42, 1273 -1280.  \\n61. Rogers, D.; Hahn, M., Extended -connectivity fingerprints. Journal of chemical information and modeling \\n2010 , 50, 742 -754.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='2010 , 50, 742 -754.  \\n62. Hert, J.; Willett, P.; Wilton, D. J.; Acklin, P.; Azzaoui, K.; Jacoby, E.; Schuffenhauer, A., Comparison of \\nfingerprint -based methods for virtual screening using multiple bioactive reference structures. Journal of \\nchemical information and compute r sciences 2004 , 44, 1177 -1185.  \\n63. Bian, Y.; Wang, J.; Jun, J. J.; Xie, X. -Q., Deep convolutional generative adversarial network (dcGAN)'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='models for screening and design of small molecules targeting cannabinoid receptors. Molecular pharmaceutics \\n2019 , 16, 4451 -4460.  \\n64. Elton, D. C.; Boukouvalas, Z.; Fuge, M. D.; Chung, P. W., Deep learning for molecular design —a review \\nof the state of the art. Molecular Systems Design & Engineering 2019 , 4, 828 -849.  \\n65. Wang, R.; Fang, X.; Lu, Y.; Yang, C. -Y.; Wang, S., Th e PDBbind database: methodologies and updates. \\nJournal of medicinal chemistry 2005 , 48, 4111 -4119.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='66. Papadatos, G.; Davies, M.; Dedman, N.; Chambers, J.; Gaulton, A.; Siddle, J.; Koks, R.; Irvine, S. A.; \\nPettersson, J.; Goncharoff, N., SureChEMBL: a larg e-scale, chemically annotated patent document database. \\nNucleic acids research 2016 , 44, D1220 -D1228.  \\n67. Gilson, M. K.; Liu, T.; Baitaluk, M.; Nicola, G.; Hwang, L.; Chong, J., BindingDB in 2015: a public database \\nfor medicinal chemistry, computational ch emistry and systems pharmacology. Nucleic acids research 2016 , 44,'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='D1045 -D1053.  \\n68. Ruddigkeit, L.; Van Deursen, R.; Blum, L. C.; Reymond, J. -L., Enumeration of 166 billion organic small \\nmolecules in the chemical universe database GDB -17. Journal of chemic al information and modeling 2012 , 52, \\n2864 -2875.  \\n69. Heller, S. R.; McNaught, A.; Pletnev, I.; Stein, S.; Tchekhovskoi, D., InChI, the IUPAC international \\nchemical identifier. Journal of cheminformatics 2015 , 7, 23.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content='70. Glen, R. C.; Bender, A.; Arnby, C. H .; Carlsson, L.; Boyer, S.; Smith, J., Circular fingerprints: flexible \\nmolecular descriptors with applications from physical chemistry to ADME. IDrugs 2006 , 9, 199.  \\n71. Pérez -Nueno, V. I.; Rabal, O.; Borrell, J. I.; Teixidó, J., APIF: a new interaction fin gerprint based on atom \\npairs and its application to virtual screening. Journal of chemical information and modeling 2009 , 49, 1245 -\\n1260.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 25}, page_content=\"1260.  \\n72. O'Boyle, N. M.; Banck, M.; James, C. A.; Morley, C.; Vandermeersch, T.; Hutchison, G. R., Open Babel: \\nAn open chemi cal toolbox. Journal of cheminformatics 2011 , 3, 33.  \\n73. Willighagen, E. L.; Mayfield, J. W.; Alvarsson, J.; Berg, A.; Carlsson, L.; Jeliazkova, N.; Kuhn, S.; Pluskal, T.; \\nRojas -Chertó, M.; Spjuth, O., The Chemistry Development Kit (CDK) v2. 0: atom typing , depiction, molecular \\nformulas, and substructure searching. Journal of cheminformatics 2017 , 9, 33.\"),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='27 \\n 74. Ambure, P.; Aher, R. B.; Roy, K. Recent advances in the open access cheminformatics toolkits, software \\ntools, workflow environments, and databases. In Computer -Aided Drug Discovery ; Springer: 2014, pp 257 -296.  \\n75. Arabie, P.; Baier, N. D.; Critchley, C. F.; Keynes, M., Studies in Classification, Data Analysis, and \\nKnowledge Organization. 2006 . \\n76. Warr, W. A., Scientific workflow systems: Pipeline Pilot and KNIME. Journal of computer -aided'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='molecular design 2012 , 26, 801 -804.  \\n77. Beisken, S.; Meinl, T.; Wiswedel, B.; de Figueiredo, L. F.; Berthold, M.; Steinbeck, C., KNIME -CDK: \\nWorkflow -driven cheminformatics. BMC bioinformatics 2013 , 14, 257.  \\n78. Saubern,  S.; Guha, R.; Baell, J. B., KNIME workflow to assess PAINS filters in SMARTS format. \\nComparison of RDKit and Indigo cheminformatics libraries. Molecular informatics 2011 , 30, 847 -850.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='79. Roughley, S. D., Five Years of the KNIME Vernalis Cheminformatics C ommunity Contribution. Current \\nmedicinal chemistry 2019 . \\n80. Abadi, M.; Barham, P.; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, \\nM. Tensorflow: A system for large -scale machine learning. In 12th {USENIX} Symposium on  Operating Systems \\nDesign and Implementation ({OSDI} 16), 2016; 2016; pp 265 -283.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='81. Etaati, L. Deep Learning Tools with Cognitive Toolkit (CNTK). In Machine Learning with Microsoft \\nTechnologies ; Springer: 2019, pp 287 -302.  \\n82. Team, T.; Al -Rfou, R.; Alai n, G.; Almahairi, A.; Angermueller, C.; Bahdanau, D.; Ballas, N.; Bastien, F.; \\nBayer, J.; Belikov, A.; Belopolsky, A.; Bengio, Y.; Bergeron, A.; Bergstra, J.; Bisson, V.; Bleecher Snyder, J.; \\nBouchard, N.; Boulanger -Lewandowski, N.; Bouthillier, X.; Zhang,  Y., Theano: A Python framework for fast'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='computation of mathematical expressions. 2016 . \\n83. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; \\nAntiga, L. PyTorch: An imperative style, high -performan ce deep learning library. In Advances in Neural \\nInformation Processing Systems, 2019; 2019; pp 8024 -8035.  \\n84. Chollet, F., In; 2015.  \\n85. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer,'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='P.; Weiss, R.; Dubourg, V., Scikit -learn: Machine learning in Python. the Journal of machine Learning research \\n2011 , 12, 2825 -2830.  \\n86. Mikolov, T.; Karafiát, M.; Burget, L.; Černocký, J.; Khudanpur, S. Recurrent neural network based \\nlanguage model. In Eleventh annual conference of the international speech communication association, 2010; \\n2010.  \\n87. Mikolov, T.; Kombrink, S.; Burget, L. ; Černocký, J.; Khudanpur, S. Extensions of recurrent neural'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='network language model. In 2011 IEEE international conference on acoustics, speech and signal processing \\n(ICASSP), 2011; IEEE: 2011; pp 5528 -5531.  \\n88. Mikolov, T.; Zweig, G. Context dependent rec urrent neural network language model. In 2012 IEEE \\nSpoken Language Technology Workshop (SLT), 2012; IEEE: 2012; pp 234 -239.  \\n89. Hanson, J.; Yang, Y.; Paliwal, K.; Zhou, Y., Improving protein disorder prediction by deep bidirectional'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='long short -term memory recurrent neural networks. Bioinformatics 2017 , 33, 685 -692.  \\n90. Cheng, J.; Dong, L.; Lapata, M., Long short -term memory -networks for machine reading. arXiv preprint \\narXiv:1601.06733 2016 . \\n91. Gupta, A.; Müller, A. T.; Huisman, B. J.; Fuchs, J. A.; Schneid er, P.; Schneider, G., Generative recurrent \\nnetworks for de novo drug design. Molecular informatics 2018 , 37, 1700111.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 26}, page_content='92. Bian, Y.; Xie, X. -Q. S., Computational fragment -based drug design: Current trends, strategies, and \\napplications. The AAPS journal 2018, 20, 59.  \\n93. Segler, M. H.; Kogej, T.; Tyrchan, C.; Waller, M. P., Generating focused molecule libraries for drug \\ndiscovery with recurrent neural networks. ACS central science 2018 , 4, 120 -131.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='28 \\n 94. Moret, M.; Friedrich, L.; Grisoni, F.; Merk, D.; Schneid er, G., Generative molecular design in low data \\nregimes. Nature Machine Intelligence 2020 , 2, 171 -180.  \\n95. Merk, D.; Friedrich, L.; Grisoni, F.; Schneider, G., De novo design of bioactive small molecules by \\nartificial intelligence. Molecular informatics 2018, 37, 1700153.  \\n96. Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y., Empirical evaluation of gated recurrent neural networks on'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='sequence modeling. arXiv preprint arXiv:1412.3555 2014 . \\n97. Zheng, S.; Yan, X.; Gu, Q.; Yang, Y.; Du, Y.; Lu, Y.; Xu, J., QBMG: qu asi-biogenic molecule generator with \\ndeep recurrent neural network. Journal of cheminformatics 2019 , 11, 5.  \\n98. Kramer, M. A., Nonlinear principal component analysis using autoassociative neural networks. AIChE \\njournal 1991 , 37, 233 -243.  \\n99. Kingma, D. P.;  Welling, M., Auto -encoding variational bayes. arXiv preprint arXiv:1312.6114 2013 .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='100.  Kingma, D. P.; Welling, M., An introduction to variational autoencoders. arXiv preprint \\narXiv:1906.02691 2019 . \\n101.  Kingma, D. P.; Mohamed, S.; Rezende, D. J.; Welling , M. Semi -supervised learning with deep generative \\nmodels. In Advances in neural information processing systems, 2014; 2014; pp 3581 -3589.  \\n102.  Khemakhem, I.; Kingma, D. P.; Hyvärinen, A., Variational autoencoders and nonlinear ica: A unifying \\nframework. arXiv preprint arXiv:1907.04809 2019 .'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='103.  Pu, Y.; Gan, Z.; Henao, R.; Yuan, X.; Li, C.; Stevens, A.; Carin, L. Variational autoencoder for deep \\nlearning of images, labels and captions. In Advances in neural information processing systems, 2016; 2016; pp \\n2352-2360.  \\n104.  Gómez -Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernández -Lobato, J. M.; Sánchez -Lengeling, B.; \\nSheberla, D.; Aguilera -Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru -Guzik, A., Automatic chemical design'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='using a data -driven continuous r epresentation of molecules. ACS central science 2018 , 4, 268 -276.  \\n105.  Blaschke, T.; Olivecrona, M.; Engkvist, O.; Bajorath, J.; Chen, H., Application of generative autoencoder \\nin de novo molecular design. Molecular informatics 2018 , 37, 1700123.  \\n106.  Sattarov, B.; Baskin, I. I.; Horvath, D.; Marcou, G.; Bjerrum, E. J.; Varnek, A., De novo molecular design'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='by combining deep autoencoder recurrent neural networks with generative topographic mapping. Journal of \\nchemical information and modeling 2019 , 59, 1182 -1196.  \\n107.  Samanta, B.; Abir, D.; Jana, G.; Chattaraj, P. K.; Ganguly, N.; Rodriguez, M. G. Nevae: A deep generative \\nmodel for molecular graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019; 2019; Vol. \\n33; pp 1110 -1117.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='33; pp 1110 -1117.  \\n108.  Simon ovsky, M.; Komodakis, N. Graphvae: Towards generation of small graphs using variational \\nautoencoders. In International Conference on Artificial Neural Networks, 2018; Springer: 2018; pp 412 -422.  \\n109.  Imrie, F.; Bradley, A. R.; van der Schaar, M.; Deane, C.  M., Deep Generative Models for 3D Linker Design. \\nJournal of Chemical Information and Modeling 2020 , 60, 1983 -1995.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='110.  Makhzani, A.; Shlens, J.; Jaitly, N.; Goodfellow, I.; Frey, B., Adversarial autoencoders. arXiv preprint \\narXiv:1511.05644 2015 . \\n111.  Kadurin, A.; Nikolenko, S.; Khrabrov, K.; Aliper, A.; Zhavoronkov, A., druGAN: an advanced generative \\nadversarial autoencoder model for de novo generation of new molecules with desired molecular properties in \\nsilico. Molecular pharmaceutics 2017 , 14, 3098 -3104.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='112.  Polykovskiy, D.; Zhebrak, A.; Vetrov, D.; Ivanenkov, Y.; Aladinskiy, V.; Mamoshina, P.; Bozdaganyan, M.; \\nAliper, A.; Zhavoronkov, A.; Kadurin, A., Entangled conditional adversarial autoencoder for de novo drug \\ndiscovery. Molecular pharmaceutics 2018, 15, 4398 -4405.  \\n113.  Shayakhmetov, R.; Kuznetsov, M.; Zhebrak, A.; Kadurin, A.; Nikolenko, S.; Aliper, A.; Polykovskiy, D., \\nMolecular Generation for Desired Transcriptome Changes With Adversarial Autoencoders. Frontiers in'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 27}, page_content='Pharmacology 2020 , 11, 269.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='29 \\n 114. Guimaraes, G. L.; Sanchez -Lengeling, B.; Outeiral, C.; Farias, P. L. C.; Aspuru -Guzik, A., Objective -\\nreinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint \\narXiv:1705.10843 2017 . \\n115.  Maziarka, Ł.; Pocha, A.; Kaczmarczyk, J.; Rataj, K.; Danel, T.; Warchoł, M., Mol -CycleGAN: a generative \\nmodel for molecular optimization. Journal of Cheminformatics 2020 , 12, 1 -18.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='116.  Méndez -Lucio, O.; Baillif, B.; Clevert, D. -A.; Rouquié, D.; Wicha rd, J., De novo generation of hit -like \\nmolecules from gene expression signatures using artificial intelligence. Nature Communications 2020 , 11, 1 -10. \\n117.  Prykhodko, O.; Johansson, S. V.; Kotsias, P. -C.; Arús -Pous, J.; Bjerrum, E. J.; Engkvist, O.; Chen, H ., A de \\nnovo molecular generation method using latent vector based generative adversarial network. Journal of \\nCheminformatics 2019 , 11, 74.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='Cheminformatics 2019 , 11, 74.  \\n118.  Huang, G.; Liu, Z.; Van Der Maaten, L.; Weinberger, K. Q. Densely connected convolutional networks. In \\nProceedi ngs of the IEEE conference on computer vision and pattern recognition, 2017; 2017; pp 4700 -4708.  \\n119.  LeCun, Y.; Bengio, Y., Convolutional networks for images, speech, and time series. The handbook of \\nbrain theory and neural networks 1995 , 3361, 1995.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='120.  Yu, D.; Wang, H.; Chen, P.; Wei, Z. Mixed pooling for convolutional neural networks. In International \\nconference on rough sets and knowledge technology, 2014; Springer: 2014; pp 364 -375.  \\n121.  Radford, A.; Metz, L.; Chintala, S., Unsupervised representatio n learning with deep convolutional \\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434 2015 . \\n122.  Zhang, H.; Goodfellow, I.; Metaxas, D.; Odena, A., Self -attention generative adversarial networks. arXiv'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='preprint arXiv:1805.08318 2018 . \\n123.  Li, C.; Wand, M. Precomputed real -time texture synthesis with markovian generative adversarial \\nnetworks. In European conference on computer vision, 2016; Springer: 2016; pp 702 -716.  \\n124.  Gao, W.; Coley, C. W., The synthesizability of molecules proposed by gen erative models. Journal of \\nChemical Information and Modeling 2020 . \\n125.  Coley, C. W.; Rogers, L.; Green, W. H.; Jensen, K. F., SCScore: synthetic complexity learned from a'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='reaction corpus. Journal of chemical information and modeling 2018 , 58, 252 -261.  \\n126. Vargesson, N., Thalidomide ‐induced teratogenesis: History and mechanisms. Birth Defects Research \\nPart C: Embryo Today: Reviews 2015 , 105, 140 -156.  \\n127.  Polishchuk, P. G.; Madzhidov, T. I.; Varnek, A., Estimation of the size of drug -like chemical space ba sed \\non GDB -17 data. Journal of computer -aided molecular design 2013 , 27, 675 -679.'),\n",
       " Document(metadata={'source': 'gan_drug_discovery.pdf', 'page': 28}, page_content='128.  Alley, E. C.; Khimulya, G.; Biswas, S.; AlQuraishi, M.; Church, G. M., Unified rational protein engineering \\nwith sequence -based deep representation learning. Nature meth ods 2019 , 16, 1315 -1322.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                                               chunk_overlap=50)\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech=\"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech=f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=5)\n",
    "text = text_splitter.create_documents([speech])\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 367, which is longer than the specified 100\n",
      "Created a chunk of size 490, which is longer than the specified 100\n",
      "Created a chunk of size 394, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI.[1][2] The concept was initially developed by Ian Goodfellow and his colleagues in June 2014.[3] In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning,[4] fully supervised learning,[5] and reinforcement learning.[6]'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='The core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically.[7] This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", \n",
    "                                      chunk_size=100, \n",
    "                                      chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 367, which is longer than the specified 100\n",
      "Created a chunk of size 490, which is longer than the specified 100\n",
      "Created a chunk of size 394, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI.[1][2] The concept was initially developed by Ian Goodfellow and his colleagues in June 2014.[3] In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech = \"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text = text_splitter.create_documents([speech])\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Header Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'header 1': 'Welcome to My HTML Page'}, page_content='This is a paragraph of text.  \\nHello world')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My HTML Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to My HTML Page</h1>\n",
    "    <p>This is a paragraph of text.</p>\n",
    "    <div>\n",
    "        <h2>Hello world</h2>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"header 1\"),\n",
    "    (\"div\", \"div\"),\n",
    "    (\"p\", \"paragraph\")\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
